{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e62dcd08-2680-437a-b10c-4c9b6b2f58d9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 1"
    }
   },
   "source": [
    "# **PHASE 3: ADVANCED ANALYTICS (Days 9-11)**\n",
    "\n",
    "## **DAY 11 (19/01/26) - Statistical Analysis & ML Prep**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7532b03a-927e-4e8a-9856-33d66f08e2ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Section 1 - Learn**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e8b751c-3188-4f16-b99b-334bf092a600",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **_1. Descriptive statistics_**\n",
    "\n",
    "Descriptive statistics are the first step in **Exploratory Data Analysis (EDA)**. In Databricks, you can generate these summaries using built-in methods that work across billions of rows without breaking a sweat.\n",
    "\n",
    "##### **1. The Quick Look: `describe()`**\n",
    "\n",
    "The `describe()` method is the fastest way to get a high-level overview of your numerical and string columns. It provides a fixed set of statistics: **count**, **mean**, **stddev**, **min**, and **max**.\n",
    "\n",
    "```python\n",
    "# Generate basic stats for all columns\n",
    "df.describe().show()\n",
    "\n",
    "# Focus on specific columns\n",
    "df.describe(\"age\", \"salary\").show()\n",
    "\n",
    "```\n",
    "\n",
    "##### **2. The Deep Dive: `summary()`**\n",
    "\n",
    "If you need more detail—specifically **percentiles**—use the `summary()` method. By default, it includes everything from `describe()` plus the **25%**, **50% (median)**, and **75%** quartiles.\n",
    "\n",
    "* **Custom Stats:** You can pass specific statistics as strings to `summary()` to limit the output.\n",
    "* **Percentiles:** You can request any arbitrary percentile (e.g., `\"95%\"`, `\"99%\"`).\n",
    "\n",
    "```python\n",
    "# Default summary (includes quartiles)\n",
    "df.summary().show()\n",
    "\n",
    "# Custom summary for specific metrics\n",
    "df.select(\"amount\").summary(\"count\", \"min\", \"50%\", \"95%\", \"max\").show()\n",
    "\n",
    "```\n",
    "\n",
    "##### **3. Manual Aggregations with `agg()`**\n",
    "\n",
    "For production pipelines or custom metrics (like **Skewness** or **Kurtosis**), you should use the `agg()` function. This is more efficient than `summary()` because it doesn't calculate metrics you don't need.\n",
    "\n",
    "```python\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "stats_df = df.agg(\n",
    "    F.count(\"user_id\").alias(\"total_users\"),\n",
    "    F.mean(\"purchase_value\").alias(\"avg_spend\"),\n",
    "    F.stddev(\"purchase_value\").alias(\"spend_volatility\"),\n",
    "    F.skewness(\"purchase_value\").alias(\"spend_skew\"),\n",
    "    F.kurtosis(\"purchase_value\").alias(\"spend_tail_risk\")\n",
    ")\n",
    "\n",
    "```\n",
    "\n",
    "##### **4. Advanced Statistics with `stat`**\n",
    "\n",
    "The `df.stat` attribute provides specialized statistical functions that aren't available in standard aggregations:\n",
    "\n",
    "* **`crosstab(col1, col2)`**: Generates a frequency table (contingency table) for two columns.\n",
    "* **`corr(col1, col2)`**: Calculates the Pearson correlation coefficient between two variables.\n",
    "* **`cov(col1, col2)`**: Calculates the sample covariance.\n",
    "* **`freqItems(cols)`**: Finds frequent items (approximate) in a large dataset using the Frequent Items algorithm.\n",
    "\n",
    "##### **Summary of Descriptive Tools**\n",
    "\n",
    "| Tool | Best For... | Output Format |\n",
    "| --- | --- | --- |\n",
    "| **`describe()`** | Quick sanity check | Row-based table (Standard stats) |\n",
    "| **`summary()`** | Distribution analysis | Row-based table (includes Percentiles) |\n",
    "| **`agg()`** | Custom/Production logic | Single-row DataFrame |\n",
    "| **`df.stat`** | Relationships & Probabilities | Varies (Tables/Numbers) |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06338551-afd0-41fe-b5d7-60242170a294",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d043ee4d-c347-477b-8152-f121b00d7b89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **_2. Hypothesis testing_**\n",
    "\n",
    "Hypothesis testing in Databricks allows you to move beyond simple descriptive stats to determine if your findings are **statistically significant** or just due to random chance.\n",
    "\n",
    "In a distributed environment, you have two primary ways to perform these tests: using the native **Spark MLlib** (for massive datasets) or **SciPy/Statsmodels** (for smaller samples or aggregated results).\n",
    "\n",
    "##### **1. Chi-Square Test (Independence)**\n",
    "\n",
    "The Chi-Square test is the primary hypothesis test built directly into **PySpark MLlib**. It is used to determine if two categorical variables are independent (e.g., \"Does a customer's region affect their preferred product category?\").\n",
    "\n",
    "* **Null Hypothesis ():** The variables are independent (no relationship).\n",
    "* **Alternative Hypothesis ():** The variables are dependent (significant relationship).\n",
    "\n",
    "```python\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.stat import ChiSquareTest\n",
    "\n",
    "# Sample data: (label, features)\n",
    "# label: categorical (e.g., Purchased=1, Browsed=0)\n",
    "# features: Vector of categorical values (e.g., [Region_ID, Gender_ID])\n",
    "data = [\n",
    "    (0.0, Vectors.dense(0, 10)),\n",
    "    (1.0, Vectors.dense(1, 20)),\n",
    "    (1.0, Vectors.dense(1, 10)),\n",
    "    (0.0, Vectors.dense(0, 20))\n",
    "]\n",
    "df = spark.createDataFrame(data, [\"label\", \"features\"])\n",
    "\n",
    "# Run the test\n",
    "result = ChiSquareTest.test(df, \"features\", \"label\").head()\n",
    "\n",
    "print(f\"p-values: {result.pValues}\")\n",
    "print(f\"Degrees of Freedom: {result.degreesOfFreedom}\")\n",
    "print(f\"Statistics: {result.statistics}\")\n",
    "\n",
    "```\n",
    "\n",
    "##### **2. T-Tests (Comparing Means)**\n",
    "\n",
    "Spark MLlib does not have a native \"distributed\" T-test. For this, you typically aggregate your data in Spark and then use the **SciPy** library.\n",
    "\n",
    "* **Independent T-Test:** Compares the means of two different groups (e.g., \"Does Group A spend more than Group B?\").\n",
    "* **Paired T-Test:** Compares the same group at different times (e.g., \"Before vs. After a marketing campaign\").\n",
    "\n",
    "```python\n",
    "import scipy.stats as stats\n",
    "\n",
    "# 1. Aggregate data in Spark\n",
    "group_a = spark.table(\"sales\").filter(\"group == 'A'\").select(\"amount\").toPandas()[\"amount\"]\n",
    "group_b = spark.table(\"sales\").filter(\"group == 'B'\").select(\"amount\").toPandas()[\"amount\"]\n",
    "\n",
    "# 2. Run the test in Python\n",
    "t_stat, p_val = stats.ttest_ind(group_a, group_b)\n",
    "\n",
    "print(f\"T-Statistic: {t_stat:.4f}\")\n",
    "print(f\"P-Value: {p_val:.4f}\")\n",
    "\n",
    "```\n",
    "\n",
    "##### **3. Bootstrapping (The \"Spark Way\")**\n",
    "\n",
    "When you have billions of rows and traditional tests (which assume a normal distribution) aren't fitting, you can use **Bootstrapping**. This involves sampling your data with replacement thousands of times to build an actual distribution of your metric.\n",
    "\n",
    "* **Why use it?** It’s mathematically simpler for distributed systems and doesn't require your data to be \"normally distributed.\"\n",
    "* **Decision:** If your 95% confidence interval from bootstrapping does not contain the \"null\" value (e.g., 0), you reject the null hypothesis.\n",
    "\n",
    "##### **4. Summary of Test Selection**\n",
    "\n",
    "| If you want to test... | Use this test... | Recommended Library |\n",
    "| --- | --- | --- |\n",
    "| **Independence of categories** | Chi-Square Test | `pyspark.ml.stat` |\n",
    "| **Difference in means (2 groups)** | T-Test | `scipy.stats` (on sampled data) |\n",
    "| **Difference in means (>2 groups)** | ANOVA | `scipy.stats` or `statsmodels` |\n",
    "| **Relationship between numbers** | Pearson/Spearman | `pyspark.ml.stat.Correlation` |\n",
    "| **Massive, non-normal data** | Bootstrapping | Custom Spark logic (sampling) |\n",
    "\n",
    "##### **Interpreting the P-Value**\n",
    "\n",
    "* **:** Reject the Null Hypothesis. There is a **statistically significant** difference/relationship.\n",
    "* **:** Fail to reject the Null Hypothesis. Any difference is likely due to **random noise**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7aab340-5657-4263-838b-2738d2005519",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc4ae1f6-7b52-4f7c-8abc-617065010a5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **_3. A/B test design_**\n",
    "\n",
    "Designing an **A/B Test** (or randomized controlled trial) in Databricks requires a combination of statistical rigor and engineering setup. Because Databricks is a unified platform, you can handle the entire lifecycle: from power analysis in a notebook to model serving and real-time tracking.\n",
    "\n",
    "##### **1. The Design Phase (Notebooks)**\n",
    "\n",
    "Before writing any code, you must establish the mathematical boundaries of your experiment.\n",
    "\n",
    "* **Formulate a Hypothesis:** Use a specific format: *\"If we change [Independent Variable], then [Dependent Variable/KPI] will increase because [Reasoning].\"*\n",
    "* **Power Analysis:** Determine your **Minimum Detectable Effect (MDE)** and required **Sample Size**.\n",
    "* ** (Significance):** Usually 0.05. The risk of a \"False Positive.\"\n",
    "* ** (Power):** Usually 0.80. The probability of detecting a real effect if it exists.\n",
    "* **MDE:** The smallest change that is actually meaningful to your business (e.g., a 2% lift in conversion).\n",
    "\n",
    "\n",
    "##### **2. Implementation: User Allocation**\n",
    "\n",
    "You must ensure that a user is **consistently** assigned to either Group A or Group B to avoid \"pollution.\"\n",
    "\n",
    "* **Hash-Based Allocation:** Instead of random numbers (which can change on refresh), hash the `user_id` and use the modulo operator. This ensures that `user_123` always sees the same variant.\n",
    "* **Experiment Metadata:** Store the assignment in a Delta table. A common schema is `(user_id, experiment_name, variant, timestamp)`.\n",
    "\n",
    "```python\n",
    "import hashlib\n",
    "\n",
    "def get_variant(user_id, experiment_name):\n",
    "    # Salt the hash with the experiment name to avoid correlation between tests\n",
    "    combined_id = f\"{user_id}_{experiment_name}\"\n",
    "    hash_val = int(hashlib.md5(combined_id.encode()).hexdigest(), 16)\n",
    "    return \"B\" if hash_val % 2 == 0 else \"A\"\n",
    "\n",
    "```\n",
    "\n",
    "##### **3. Engineering: The \"Silver\" Experiment Table**\n",
    "\n",
    "In your **Silver layer**, create a table that joins user events with their experiment assignments. This is where the actual analysis happens.\n",
    "\n",
    "* **Key Columns:** `user_id`, `variant`, `metric_value` (e.g., total spend), `is_converted`.\n",
    "* **Unity Catalog Tip:** Use **Tags** on your experiment tables to track ownership and \"Start/End\" dates.\n",
    "\n",
    "\n",
    "##### **4. Model-Based A/B Testing (MLflow)**\n",
    "\n",
    "If you are testing two Machine Learning models (e.g., two different recommendation algorithms), Databricks provides built-in **Traffic Splitting** in Model Serving.\n",
    "\n",
    "* **MLflow Deployment:** You can deploy two models to a single \"Served Endpoint\" and tell Databricks to send 50% of traffic to `Model_A` and 50% to `Model_B`.\n",
    "* **Inference Logging:** Enable **Inference Tables** to automatically record every prediction and the model ID used. You can later join this with \"Ground Truth\" data (actual purchases) to see which model performed better.\n",
    "\n",
    "\n",
    "##### **5. Post-Test Analysis (SQL & Stats)**\n",
    "\n",
    "Once the test duration is complete (at least 1-2 full business cycles):\n",
    "\n",
    "* **Check for Skew:** Ensure that the number of users in A and B is roughly equal. If it's 70/30, your allocation logic might be broken.\n",
    "* **Calculate P-Value:** Use **SciPy** or **Statsmodels** in a notebook to check for significance.\n",
    "* **Visualization:** Build a **Databricks SQL Dashboard** to track the lift in real-time. Use a \"Counter\" for the P-Value and a \"Bar Chart\" for the metric comparison.\n",
    "\n",
    "##### **Best Practices Checklist**\n",
    "\n",
    "* **Don't \"Peek\":** Avoid stopping a test early just because it looks significant at day 3. Wait until you reach your pre-determined sample size.\n",
    "* **Run Simultaneously:** Never run Group A one week and Group B the next. External factors (holidays, sales) will ruin your data.\n",
    "* **Standardize Metrics:** Use a single \"Source of Truth\" for metrics like `Revenue` across all experiments to ensure results are comparable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64198984-5d5b-4a51-936b-a237fd10e5be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e6816f6-7d6f-4983-81a3-107551655a99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **_4. Feature engineering_**\n",
    "\n",
    "**Feature Engineering** is the process of transforming raw data into \"features\" that better represent the underlying problem to machine learning models, thereby improving their accuracy.\n",
    "\n",
    "In Databricks, feature engineering often mirrors the **Silver to Gold** transition in the Medallion architecture, where data is moved from a cleaned state to a highly optimized, domain-specific state.\n",
    "\n",
    "##### **1. Common Feature Engineering Techniques**\n",
    "\n",
    "Feature engineering can be categorized into four main processes:\n",
    "\n",
    "| Process | Technique | Description |\n",
    "| --- | --- | --- |\n",
    "| **Transformation** | **Scaling / Normalization** | Adjusting numeric values to a common range (e.g.,  to ) so large numbers don't dominate the model. |\n",
    "| **Encoding** | **One-Hot Encoding** | Converting categorical strings (like \"Red\", \"Blue\") into binary columns (s and s). |\n",
    "| **Creation** | **Aggregation** | Creating features like `average_spend_30d` or `total_logins_last_week` using window functions. |\n",
    "| **Extraction** | **PCA / Dimensionality** | Reducing the number of features by combining variables into principal components. |\n",
    "\n",
    "\n",
    "##### **2. Practical PySpark Examples**\n",
    "\n",
    "###### **A. Handling Categorical Data (StringIndexer & OneHotEncoder)**\n",
    "\n",
    "Machine learning models require numerical input. You first convert strings to indices, then into binary vectors.\n",
    "\n",
    "```python\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "\n",
    "# 1. Convert Category Name to Index (0, 1, 2...)\n",
    "indexer = StringIndexer(inputCol=\"category\", outputCol=\"category_index\")\n",
    "df_indexed = indexer.fit(df).transform(df)\n",
    "\n",
    "# 2. Convert Index to Binary Vectors\n",
    "encoder = OneHotEncoder(inputCol=\"category_index\", outputCol=\"category_vec\")\n",
    "df_encoded = encoder.fit(df_indexed).transform(df_indexed)\n",
    "\n",
    "```\n",
    "\n",
    "###### **B. Time-Series Window Aggregations**\n",
    "\n",
    "Creating features that capture trends over time is crucial for behavioral prediction (like churn).\n",
    "\n",
    "```python\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Define a window for each customer looking back 7 days\n",
    "window_spec = Window.partitionBy(\"customer_id\").orderBy(F.col(\"timestamp\").cast(\"long\")).rangeBetween(-7*86400, 0)\n",
    "\n",
    "# Create a \"Rolling Average Spend\" feature\n",
    "df_features = df.withColumn(\"avg_spend_7d\", F.avg(\"amount\").over(window_spec))\n",
    "\n",
    "```\n",
    "\n",
    "##### **3. The Databricks Feature Store**\n",
    "\n",
    "In 2026, Databricks uses **Unity Catalog** as a built-in **Feature Store**. This allows you to:\n",
    "\n",
    "* **Discover & Reuse:** Search for existing features created by other teams to avoid \"reinventing the wheel.\"\n",
    "* **Point-in-Time Joins:** Automatically join features based on the correct timestamp to prevent **data leakage** (accidentally using \"future\" information to train a model).\n",
    "* **Lineage Tracking:** See exactly which models are using a specific feature.\n",
    "\n",
    "```python\n",
    "from databricks.feature_engineering import FeatureEngineeringClient\n",
    "\n",
    "fe = FeatureEngineeringClient()\n",
    "\n",
    "# Create a Feature Table in Unity Catalog\n",
    "fe.create_table(\n",
    "    name=\"catalog.schema.customer_features\",\n",
    "    primary_keys=[\"customer_id\"],\n",
    "    df=df_features,\n",
    "    description=\"Customer behavioral and trend features\"\n",
    ")\n",
    "\n",
    "```\n",
    "\n",
    "##### **Best Practices**\n",
    "\n",
    "* **Avoid Data Leakage:** Ensure your features only use data available *before* the time of the event you are trying to predict.\n",
    "* **VectorAssembler:** Most Spark ML algorithms expect a single column called `features` containing a vector. Use `VectorAssembler` as the final step to combine all your engineered columns.\n",
    "* **Automate with DLT:** Use **Delta Live Tables** to keep your feature tables updated incrementally as new raw data arrives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8addbe16-6bbb-4ca7-b2cc-0668ba62bca3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20f23c91-cbe8-4f2e-9e7e-9f255237199a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Practice**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9f15bb0-92a0-485b-bd7b-7bb97e6afcc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfb66933-8549-4f61-9ac8-8d34e5f25bc3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def load_ecommerce_dataset(Month_name):\n",
    "    df = spark.read.csv(f\"/Volumes/workspace/ecommerce/ecommerce_data/2019-{Month_name}.csv\", header=True, inferSchema=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c21086e-7ca8-4253-bb99-63e35b3ee023",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df_n = load_ecommerce_dataset(\"Nov\")\n",
    "df_o = load_ecommerce_dataset(\"Oct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d8e9a32-301b-44f8-9c25-e97c74c285cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### **1. Calculate statistical summaries** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e15da70-2cd4-4db6-afeb-aa618ed6aaf5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Basic summary: Count, Mean, Stddev, Min, Max\n",
    "display(df_o.select(\"price\").describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d48c73b-db9c-4b2c-9272-df9008b573ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3438530d-e61d-4fb2-bcb0-1cfae0e7a36e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### **2. Test hypotheses (weekday vs weekend)** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c599c04-2228-423b-a454-c6f2df98a91f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### **_hypothesis is: \"The average price of items purchased on weekends is significantly higher than on weekdays.\"_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23b8c73b-4564-44f3-b51e-07bbb033e8e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9bbd65b-9995-4309-8b93-a7358b667d05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1 is Sunday, 7 is Saturday in PySpark\n",
    "df_hypo = df_o.filter(\"event_type = 'purchase'\") \\\n",
    "    .withColumn(\"day_of_week\", F.dayofweek(\"event_time\")) \\\n",
    "    .withColumn(\"is_weekend\", F.when(F.col(\"day_of_week\").isin(1, 7), \"Weekend\")\n",
    "                               .otherwise(\"Weekday\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "915f9fef-c82b-41f5-b68c-011ab0655ad9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# View basic distribution\n",
    "display(df_hypo.groupBy(\"is_weekend\").agg(\n",
    "    F.count(\"*\").alias(\"total_purchases\"),\n",
    "    F.avg(\"price\").alias(\"avg_price\"),\n",
    "    F.stddev(\"price\").alias(\"std_dev\")\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52136cc2-d823-47ae-b06f-08137966d259",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aaa8f935-21e4-4d72-a026-f560a6a9cca5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Extract the price arrays for both groups\n",
    "weekday_prices = df_hypo.filter(\"is_weekend = 'Weekday'\").select(\"price\").toPandas()[\"price\"]\n",
    "weekend_prices = df_hypo.filter(\"is_weekend = 'Weekend'\").select(\"price\").toPandas()[\"price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab74e5d6-dc92-4f83-a78a-4055e1f9e0bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Perform T-Test\n",
    "t_stat, p_value = stats.ttest_ind(weekday_prices, weekend_prices, equal_var=False)\n",
    "\n",
    "print(f\"T-statistic: {t_stat}\")\n",
    "print(f\"P-value: {p_value}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(\"Result: Statistically Significant. We REJECT the null hypothesis.\")\n",
    "else:\n",
    "    print(\"Result: Not Significant. We FAIL TO REJECT the null hypothesis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe80cddd-fb4c-40a6-a1be-b6dec4acd9a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2efc4de4-eb23-4d27-badc-16a95bded902",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### **3. Identify Correlations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6342bec-fc8f-4cb1-ac68-f25f4f27f440",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Correlation\n",
    "df_hypo.stat.corr(\"price\", \"category_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0a3de81-a1f4-4f45-8f48-2af1419ebc61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e87107b-1962-410b-add0-826e0f4e29c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### __4. Engineering features For ML__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91bc6f73-2a03-4d32-a7df-144d5f581a9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74a7589d-f0b5-4f9e-ae3a-5dc18f7e435c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Feature engineering\n",
    "features = df_o.withColumn(\"hour\", F.hour(\"event_time\")) \\\n",
    "    .withColumn(\"day_of_week\", F.dayofweek(\"event_time\")) \\\n",
    "    .withColumn(\"price_log\", F.log(F.col(\"price\")+1)) \\\n",
    "    .withColumn(\"time_since_first_view\",\n",
    "        F.unix_timestamp(\"event_time\") -\n",
    "        F.unix_timestamp(F.first(\"event_time\").over(Window.partitionBy(\"user_id\").orderBy(\"event_time\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bba186c-804e-4454-9b3f-ba491e0440ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(features.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d50dc071-2109-49a7-a91a-ce54a72c564d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49b7e531-86e6-497b-a178-6bf8cd8f1af9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Resources**\n",
    "- [ML](https://spark.apache.org/docs/latest/ml-guide.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e6eb3fc-5a0b-4de3-a400-fcc80f5b2d9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "----"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5202199112327274,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Day_11",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}