{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e62dcd08-2680-437a-b10c-4c9b6b2f58d9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 1"
    }
   },
   "source": [
    "# **PHASE 4: AI & ML (Days 12-14)**\n",
    "\n",
    "## **DAY 13 (21/01/26) - Model Comparison & Feature Engineering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7532b03a-927e-4e8a-9856-33d66f08e2ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Section 1 - Learn**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e8b751c-3188-4f16-b99b-334bf092a600",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **_1. Training multiple models_**\n",
    "\n",
    "Training multiple models in Databricks can range from testing a few different algorithms to training thousands of models for different \"groups\" (like a separate sales model for every store). Depending on your scale, you should choose one of the following three patterns.\n",
    "\n",
    "##### **1. Hyperparameter Tuning (Parallel Search)**\n",
    "\n",
    "If you are training one type of model but want to find the **best settings** (e.g., testing 100 different `max_depth` values), use a tuning library that integrates with Spark.\n",
    "\n",
    "* **Optuna / Ray Tune (Modern Standard):** These libraries allow you to distribute trials across the workers of your cluster. They automatically log every trial to **MLflow** so you can see which \"Run\" won.\n",
    "* **SparkTrials (Hyperopt):** A legacy but still common way to use Spark to run many small model trainings in parallel.\n",
    "\n",
    "> **Key Setting:** Set the `parallelism` parameter to match the number of CPUs in your cluster to ensure you aren't training them one by one.\n",
    "\n",
    "##### **2. Grouped Model Training (The \"Many Models\" Pattern)**\n",
    "\n",
    "If you need to train a separate model for every **Region**, **Store**, or **Product ID**, use the **Pandas UDF (User Defined Function)** pattern. This is \"embarrassingly parallel\" and highly efficient.\n",
    "\n",
    "* **How it works:** Spark splits your giant table into \"groups.\" Each group is sent to a worker node as a **Pandas DataFrame**.\n",
    "* **The Logic:** Your worker node trains the model (using `scikit-learn`, `prophet`, etc.) and logs the results.\n",
    "* **Scale:** This method can be used to train **thousands of models** in minutes.\n",
    "\n",
    "```python\n",
    "# Example: Training a separate model for each 'store_id'\n",
    "def train_store_model(pdf: pd.DataFrame) -> pd.DataFrame:\n",
    "    store_id = pdf['store_id'].iloc[0]\n",
    "    with mlflow.start_run(run_name=f\"Store_{store_id}\", nested=True):\n",
    "        # ... Train your model here ...\n",
    "        mlflow.sklearn.log_model(model, \"model\")\n",
    "    return pd.DataFrame({'store_id': [store_id], 'status': ['Success']})\n",
    "\n",
    "# Apply in parallel across the cluster\n",
    "results = (df.groupBy(\"store_id\")\n",
    "             .applyInPandas(train_store_model, schema=\"store_id long, status string\"))\n",
    "\n",
    "```\n",
    "\n",
    "##### **3. Automated Model Selection (AutoML)**\n",
    "\n",
    "If you don't want to write any training code, use **Databricks AutoML**.\n",
    "\n",
    "* **Process:** You point it at a table and tell it what you want to predict.\n",
    "* **Results:** It will automatically try dozens of different algorithms (XGBoost, LightGBM, Random Forest) with different parameters.\n",
    "* **Transparency:** It generates a **source-code notebook** for every model it tests, so you can see exactly how the \"best\" model was built and customize it.\n",
    "\n",
    "##### **Choosing Your Strategy**\n",
    "\n",
    "| Scenario | Recommended Approach | Library/Tool |\n",
    "| --- | --- | --- |\n",
    "| **Optimize one model** | Hyperparameter Tuning | Optuna or Ray Tune |\n",
    "| **Train 100+ distinct models** | Grouped Map UDF | `applyInPandas()` |\n",
    "| **Don't know which model is best** | AutoML | Databricks AutoML UI |\n",
    "| **Deep Learning (Multi-node)** | Distributed Training | `TorchDistributor` / Horovod |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06338551-afd0-41fe-b5d7-60242170a294",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d043ee4d-c347-477b-8152-f121b00d7b89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **_2. Hyperparameter tuning_**\n",
    "\n",
    "Hyperparameter tuning is the process of finding the optimal settings (hyperparameters) for your machine learning model to maximize performance. Unlike model weights, which are learned during training, hyperparameters must be set beforehand.\n",
    "\n",
    "In Databricks, hyperparameter tuning is primarily executed using **Optuna** or **Ray Tune** for modern workloads, while **Hyperopt** remains common for legacy Spark-native tuning.\n",
    "\n",
    "##### **1. Core Tuning Strategies**\n",
    "\n",
    "| Strategy | Logic | Efficiency |\n",
    "| --- | --- | --- |\n",
    "| **Grid Search** | Tests every possible combination in a fixed grid. | **Low** (Brute force) |\n",
    "| **Random Search** | Picks random combinations from a range. | **Medium** (Better for large spaces) |\n",
    "| **Bayesian Optimization** | Uses previous results to predict which settings will work best next. | **High** (Smart & fast) |\n",
    "| **Hyperband** | Starts many versions with small data and \"kills\" losers early. | **Very High** (Budget-friendly) |\n",
    "\n",
    "##### **2. Optuna + MLflow (The Modern Standard)**\n",
    "\n",
    "As of 2026, **Optuna** is the recommended tool for tuning in Databricks. It integrates deeply with **MLflow** using the `MlflowStorage` class, allowing you to use your Databricks tracking server as the database for your search.\n",
    "\n",
    "###### **Key Feature: Parallel Tuning with `MlflowSparkStudy`**\n",
    "\n",
    "You can distribute your tuning trials across your entire Spark cluster to finish in minutes instead of hours.\n",
    "\n",
    "```python\n",
    "import optuna\n",
    "from mlflow.pyspark.optuna.study import MlflowSparkStudy\n",
    "\n",
    "def objective(trial):\n",
    "    # Define the search space\n",
    "    lr = trial.suggest_float(\"learning_rate\", 1e-5, 1e-1, log=True)\n",
    "    depth = trial.suggest_int(\"max_depth\", 2, 32)\n",
    "    \n",
    "    # ... Train and evaluate your model here ...\n",
    "    return accuracy\n",
    "\n",
    "# Run 50 trials in parallel across 4 Spark worker slots\n",
    "study = MlflowSparkStudy(study_name=\"my-tuning-exp\")\n",
    "study.optimize(objective, n_trials=50, n_jobs=4)\n",
    "\n",
    "```\n",
    "\n",
    "##### **3. Scaling with SparkTrials (Hyperopt)**\n",
    "\n",
    "For teams using the **Hyperopt** library, Databricks provides the `SparkTrials` class. This class tells Hyperopt to distribute each \"trial\" (individual model training) as a separate Spark job.\n",
    "\n",
    "* **Best for:** Tuning single-node libraries like `scikit-learn` or `XGBoost` across a large cluster.\n",
    "* **Automatic Tracking:** If you have MLflow autologging enabled, every trial is automatically recorded as a \"child run\" in the MLflow UI.\n",
    "\n",
    "##### **4. Best Practices for Tuning**\n",
    "\n",
    "* **Logarithmic Scales:** For parameters like `learning_rate` or `regularization`, always use a **logarithmic scale** (e.g., `1e-5` to `1e-1`) rather than a linear one to explore different orders of magnitude.\n",
    "* **Pruning:** Use Optuna's **MedianPruner**. It automatically stops \"bad\" trials halfway through their training if they look significantly worse than previous successful trials, saving you compute cost.\n",
    "* **Use GPU Clusters sparingly:** If your individual trial doesn't *require* a GPU (e.g., small Random Forest), use a CPU-based cluster for tuning to save money. Save GPUs for the final training of the \"best\" model.\n",
    "* **Parallelism vs. Adaptiveness:**\n",
    "* **High Parallelism:** Faster results, but the \"Bayesian\" brain has less time to learn from previous results.\n",
    "* **Low Parallelism:** Slower, but usually finds a slightly more optimal model because each step is more informed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7aab340-5657-4263-838b-2738d2005519",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc4ae1f6-7b52-4f7c-8abc-617065010a5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **_3. Feature importance_**\n",
    "\n",
    "Feature importance tells you which variables have the most \"influence\" on your model's predictions. In Databricks, there are three main ways to calculate and visualize this, ranging from fast built-in metrics to mathematically rigorous \"game theory\" approaches.\n",
    "\n",
    "##### **1. Built-in (Model-Specific) Importance**\n",
    "\n",
    "Most tree-based models (like **Random Forest**, **XGBoost**, or **GBT**) automatically calculate feature importance during training based on **Gini Impurity** or **Gain**.\n",
    "\n",
    "* **Logic:** It sums up how much each feature improved the model's accuracy (or reduced \"entropy\") across all splits in all trees.\n",
    "* **Pros:** Extremely fast; zero extra computation required.\n",
    "* **Cons:** Can be biased toward high-cardinality features (like IDs or timestamps) even if they aren't truly predictive.\n",
    "\n",
    "```python\n",
    "# For Spark ML models\n",
    "importance_scores = model.featureImportances\n",
    "print(importance_scores)\n",
    "\n",
    "# For XGBoost\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import plot_importance\n",
    "plot_importance(xgb_model)\n",
    "plt.show()\n",
    "\n",
    "```\n",
    "\n",
    "##### **2. Permutation Feature Importance (Model-Agnostic)**\n",
    "\n",
    "This is a more robust technique that works for **any** model (not just trees).\n",
    "\n",
    "* **Logic:** You take a single feature and \"shuffle\" its values (permute them) while leaving the others untouched. If the model's performance crashes, that feature was important. If performance stays the same, the model wasn't really using that feature.\n",
    "* **Pros:** Reliable; avoids the \"high-cardinality bias\" of built-in methods.\n",
    "* **Cons:** Computationally expensive because it requires re-evaluating the model many times.\n",
    "\n",
    "##### **3. SHAP Values (The \"Gold Standard\")**\n",
    "\n",
    "**SHAP (SHapley Additive exPlanations)** is the modern standard for model interpretability in Databricks. It uses game theory to assign a \"fair\" contribution value to each feature for every single prediction.\n",
    "\n",
    "* **Global Importance:** Shows which features matter most across the entire dataset.\n",
    "* **Local Importance:** Explains **why** a specific customer was predicted to churn (e.g., \"The high monthly bill increased the risk, while the long tenure decreased it\").\n",
    "\n",
    "```python\n",
    "import shap\n",
    "# Use TreeExplainer for fast results on XGBoost/RandomForest\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# Visualize the top features\n",
    "shap.summary_plot(shap_values, X_test)\n",
    "\n",
    "```\n",
    "\n",
    "##### **Summary Table**\n",
    "\n",
    "| Method | Speed | Rigor | Use Case |\n",
    "| --- | --- | --- | --- |\n",
    "| **Built-in** | Instant | Low | Quick sanity check during development. |\n",
    "| **Permutation** | Slow | Medium | Verifying feature selection for non-tree models. |\n",
    "| **SHAP** | Moderate | **High** | Production explainability and business stakeholder reporting. |\n",
    "\n",
    "##### **Pro-Tip: Scaling SHAP with Spark**\n",
    "\n",
    "For massive datasets, calculating SHAP values can be slow. You can use a **Pandas UDF** in Databricks to distribute the SHAP calculations across all nodes in your cluster, making it possible to explain millions of rows in minutes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64198984-5d5b-4a51-936b-a237fd10e5be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e6816f6-7d6f-4983-81a3-107551655a99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **_4. Spark ML Pipelines_**\n",
    "\n",
    "A **Spark ML Pipeline** is a high-level API that allows you to combine multiple data transformers and a machine learning estimator into a single, cohesive workflow.\n",
    "\n",
    "Think of it like an **assembly line**: raw data enters at one end, passes through various cleaning and feature engineering stations, and emerges as a trained model at the other.\n",
    "\n",
    "##### **1. Core Components**\n",
    "\n",
    "A pipeline consists of a sequence of \"stages,\" which are typically one of two types:\n",
    "\n",
    "* **Transformers:** Algorithms that convert one DataFrame into another (e.g., `StringIndexer`, `VectorAssembler`, or a trained Model). They implement the `.transform()` method.\n",
    "* **Estimators:** Algorithms that \"learn\" from data to produce a Transformer (e.g., `LogisticRegression` or `RandomForest`). They implement the `.fit()` method.\n",
    "\n",
    "##### **2. Why Use Pipelines?**\n",
    "\n",
    "* **Consistency:** Pipelines ensure that the exact same preprocessing steps applied to your **Training** data are applied to your **Test** or **Production** data, preventing \"training-serving skew.\"\n",
    "* **Simplified Code:** Instead of managing 10 different intermediate DataFrames, you manage one `Pipeline` object.\n",
    "* **Tuning:** You can pass the entire Pipeline into a cross-validator to find the best hyperparameters for the preprocessing steps and the model simultaneously.\n",
    "\n",
    "##### **3. Typical Pipeline Structure**\n",
    "\n",
    "A standard pipeline often follows this sequence:\n",
    "\n",
    "1. **StringIndexer:** Converts categorical strings to numbers.\n",
    "2. **OneHotEncoder:** Converts those numbers into binary vectors.\n",
    "3. **VectorAssembler:** Combines all feature columns into a single \"features\" vector.\n",
    "4. **Estimator:** The actual ML algorithm (e.g., `DecisionTreeClassifier`).\n",
    "\n",
    "```python\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# Define the stages\n",
    "indexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\n",
    "assembler = VectorAssembler(inputCols=[\"categoryIndex\", \"age\", \"income\"], outputCol=\"features\")\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "\n",
    "# Construct the pipeline\n",
    "pipeline = Pipeline(stages=[indexer, assembler, rf])\n",
    "\n",
    "# Train the entire assembly line with one command\n",
    "model = pipeline.fit(train_df)\n",
    "\n",
    "# Make predictions (the model 'remembers' the indexing and assembly steps)\n",
    "predictions = model.transform(test_df)\n",
    "\n",
    "```\n",
    "\n",
    "##### **4. Persistence (Saving/Loading)**\n",
    "\n",
    "One of the biggest advantages is that you can save the entire pipeline to disk. This includes the string mappings and the vector logic.\n",
    "\n",
    "* **Save:** `model.save(\"s3://my-bucket/models/rf_pipeline\")`\n",
    "* **Load:** `loaded_model = PipelineModel.load(\"s3://my-bucket/models/rf_pipeline\")`\n",
    "\n",
    "##### **5. Best Practices**\n",
    "\n",
    "* **Pipeline vs. PipelineModel:** Use `Pipeline` for the untrained workflow and `PipelineModel` for the workflow that has already been \"fitted\" to data.\n",
    "* **Keep it Modular:** If you have very heavy preprocessing (like cleaning 100TB of raw logs), do that in a separate **Silver-to-Gold** ETL job rather than inside the ML pipeline.\n",
    "* **Inference Tables:** When deploying a Pipeline in Databricks Model Serving, use **Inference Tables** to monitor how the individual stages of your pipeline are performing in real-time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8addbe16-6bbb-4ca7-b2cc-0668ba62bca3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20f23c91-cbe8-4f2e-9e7e-9f255237199a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Practice**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfb66933-8549-4f61-9ac8-8d34e5f25bc3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59a55c5b-9fcc-4bb7-84c1-ecd0336e5a99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c21086e-7ca8-4253-bb99-63e35b3ee023",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mThe Python process exited with exit code 137 (SIGKILL: Killed). This may have been caused by an OOM error. Check your command's memory usage.\u001B[0m\n",
       "\u001B[0;31m\u001B[0m\n",
       "\u001B[0;31m\u001B[0m\n",
       "\u001B[0;31m\u001B[0m\n",
       "\u001B[0;31mThe last 10 KB of the process's stderr and stdout can be found below. See driver logs for full logs.\u001B[0m\n",
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mLast messages on stderr:\u001B[0m\n",
       "\u001B[0;31mWed Jan 21 12:55:39 2026 Connection to spark from PID  2587\u001B[0m\n",
       "\u001B[0;31mWed Jan 21 12:55:39 2026 Initialized gateway on port 43459\u001B[0m\n",
       "\u001B[0;31mWed Jan 21 12:55:39 2026 Connected to spark.\u001B[0m\n",
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mLast messages on stdout:\u001B[0m\n",
       "\u001B[0;31mNOTE: When using the `ipython kernel` entry point, Ctrl-C will not work.\u001B[0m\n",
       "\u001B[0;31m\u001B[0m\n",
       "\u001B[0;31mTo exit, you will have to explicitly quit this process, by either sending\u001B[0m\n",
       "\u001B[0;31m\"quit\" from a client, or using Ctrl-\\ in UNIX-like environments.\u001B[0m\n",
       "\u001B[0;31m\u001B[0m\n",
       "\u001B[0;31mTo read more about this, see https://github.com/ipython/ipython/issues/2049\u001B[0m\n",
       "\u001B[0;31m\u001B[0m\n",
       "\u001B[0;31m\u001B[0m\n",
       "\u001B[0;31mTo connect another client to this kernel, use:\u001B[0m\n",
       "\u001B[0;31m    --existing /databricks/kernel-connections/d1fc18431aaeb92eb3a776543c1fb6826341c751349c83b0a97c8dc37e1d9278.json\u001B[0m\n",
       "\u001B[0;31m1\u001B[0m"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": ""
       },
       "removedWidgets": [],
       "sqlProps": {
        "breakingChangeInfo": null,
        "errorClass": "PYTHON_KERNEL_UNRESPONSIVE",
        "pysparkCallSite": null,
        "pysparkFragment": null,
        "pysparkSummary": null,
        "sqlState": "XXD02",
        "stackTrace": null,
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mThe Python process exited with exit code 137 (SIGKILL: Killed). This may have been caused by an OOM error. Check your command's memory usage.\u001B[0m\n\u001B[0;31m\u001B[0m\n\u001B[0;31m\u001B[0m\n\u001B[0;31m\u001B[0m\n\u001B[0;31mThe last 10 KB of the process's stderr and stdout can be found below. See driver logs for full logs.\u001B[0m\n\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mLast messages on stderr:\u001B[0m\n\u001B[0;31mWed Jan 21 12:55:39 2026 Connection to spark from PID  2587\u001B[0m\n\u001B[0;31mWed Jan 21 12:55:39 2026 Initialized gateway on port 43459\u001B[0m\n\u001B[0;31mWed Jan 21 12:55:39 2026 Connected to spark.\u001B[0m\n\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mLast messages on stdout:\u001B[0m\n\u001B[0;31mNOTE: When using the `ipython kernel` entry point, Ctrl-C will not work.\u001B[0m\n\u001B[0;31m\u001B[0m\n\u001B[0;31mTo exit, you will have to explicitly quit this process, by either sending\u001B[0m\n\u001B[0;31m\"quit\" from a client, or using Ctrl-\\ in UNIX-like environments.\u001B[0m\n\u001B[0;31m\u001B[0m\n\u001B[0;31mTo read more about this, see https://github.com/ipython/ipython/issues/2049\u001B[0m\n\u001B[0;31m\u001B[0m\n\u001B[0;31m\u001B[0m\n\u001B[0;31mTo connect another client to this kernel, use:\u001B[0m\n\u001B[0;31m    --existing /databricks/kernel-connections/d1fc18431aaeb92eb3a776543c1fb6826341c751349c83b0a97c8dc37e1d9278.json\u001B[0m\n\u001B[0;31m1\u001B[0m"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Prepare data\n",
    "df = spark.table(\"workspace.gold.products\").limit(10000).toPandas()\n",
    "X = df[[\"views\", \"cart_adds\"]]\n",
    "y = df[\"purchases\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f44d3e74-876d-4ca3-9738-e97fa43041af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"linear\": LinearRegression(),\n",
    "    \"decision_tree\": DecisionTreeRegressor(max_depth=5),\n",
    "    \"random_forest\": RandomForestRegressor(n_estimators=100)\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    with mlflow.start_run(run_name=f\"{name}_model\"):\n",
    "        mlflow.log_param(\"model_type\", name)\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        score = model.score(X_test, y_test)\n",
    "\n",
    "        mlflow.log_metric(\"r2_score\", score)\n",
    "        mlflow.sklearn.log_model(model, \"model\")\n",
    "\n",
    "        print(f\"{name}: RÂ² = {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4889721-a90e-4471-9679-e38542b04574",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Spark ML Pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression as SparkLR\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[\"views\",\"cart_adds\"], outputCol=\"features\")\n",
    "lr = SparkLR(featuresCol=\"features\", labelCol=\"purchases\")\n",
    "pipeline = Pipeline(stages=[assembler, lr])\n",
    "\n",
    "spark_df = spark.table(\"gold.products\")\n",
    "train, test = spark_df.randomSplit([0.8, 0.2])\n",
    "model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c599c04-2228-423b-a454-c6f2df98a91f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **_Resources_**\n",
    "\n",
    "- [Spark ML](https://spark.apache.org/docs/latest/ml-classification-regression.html)\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5202199112327274,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Day_13",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}