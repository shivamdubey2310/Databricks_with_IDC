{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d587ec41-cb91-4b87-affb-1fdbf829cc2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# **PHASE 1: FOUNDATION (Days 1-4)**\n",
    "\n",
    "## **DAY 1 (09/01/26)– _Platform Setup & First Steps_**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a28bd52-586f-4500-a484-3164457bb7ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### __Section - 1 - Learn__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a06804c-42d5-4de0-a9d8-5ba9b0274ef5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **_1. Why Databricks vs Pandas/Hadoop?_**\n",
    "\n",
    "* **Massive Scalability:** Unlike Pandas, which is limited by a single machine's RAM, Databricks uses **distributed computing** to process petabytes of data across clusters.\n",
    "* **Superior Speed:** Databricks processes data **in-memory**, making it up to 100x faster than Hadoop’s MapReduce, which relies on slow disk-based reading and writing.\n",
    "* **Zero Infrastructure Overhead:** As a fully **managed cloud service**, Databricks eliminates the \"Hadoop tax\"—the massive effort required to manually configure and maintain complex server clusters.\n",
    "* **Unified Environment:** It bridges the gap between teams by allowing Data Engineers (SQL/Scala) and Data Scientists (Python/R) to collaborate within the same **shared notebooks**.\n",
    "* **ACID Reliability:** Through **Delta Lake**, Databricks adds a layer of data integrity (ACID transactions) that prevents data corruption, a common issue in traditional Hadoop data lakes.\n",
    "* **Cost-Efficient Auto-scaling:** Databricks automatically scales clusters up for heavy workloads and **shuts them down** when idle, whereas Hadoop clusters often sit idle while still incurring costs.\n",
    "* **Production-Ready AI:** It includes built-in integration with **MLflow**, making it much easier to move from a small Python prototype to a global machine learning model than it is in a Hadoop environment.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a0782861-f0f6-4a64-8927-b94a2053f889",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### _**2. Lakehouse architecture basics**_\n",
    "\n",
    "A **Data Lakehouse** is a modern architecture that merges the low-cost, flexible storage of a **Data Lake** with the high-performance management and transactions of a **Data Warehouse**.\n",
    "\n",
    "Here are the 7 key pillars of Lakehouse architecture:\n",
    "\n",
    "* **Unified Storage:** It acts as a single source of truth, storing **structured** (tables), **semi-structured** (JSON/XML), and **unstructured** (images/video) data in one place, eliminating data silos.\n",
    "* **ACID Transactions:** Using layers like Delta Lake, it ensures that data operations (Inserts, Updates, Deletes) are reliable—if a job fails halfway, it won't leave your data in a corrupted, \"half-written\" state.\n",
    "* **Decoupled Compute & Storage:** You can scale your storage (like AWS S3 or Azure Data Lake) and your processing power (Databricks clusters) **independently**, which is significantly more cost-effective than traditional databases.\n",
    "* **Schema Enforcement:** It prevents \"data swamp\" issues by rejecting data that doesn't fit a predefined format, ensuring high data quality for downstream BI and analytics.\n",
    "* **Support for Diverse Workloads:** One platform handles everything—**Data Engineering** (ETL), **BI/SQL reporting** (Dashboards), and **Machine Learning** (training models)—without needing to move data between different systems.\n",
    "* **Open Data Formats:** Data is stored in open-source, non-proprietary formats like **Parquet**. This prevents vendor lock-in and allows other tools to read the data directly.\n",
    "* **The Medallion Framework:** It typically organizes data into three logical layers to manage quality:\n",
    "  * **Bronze** (Raw data)\n",
    "  * **Silver** (Cleaned/Filtered)\n",
    "  * **Gold** (Aggregated/Business-ready).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9adeffe-57e0-44a4-9810-5955d3578011",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **_3. Databricks workspace structure_**\n",
    "\n",
    "A Databricks Workspace is the centralized environment where teams collaborate. It is structured into three distinct \"planes\" that separate the user interface, the heavy processing, and the storage.\n",
    "\n",
    "Here is the structure broken down into its 7 core components:\n",
    "\n",
    "##### 1. The Dual-Plane Architecture\n",
    "\n",
    "* **Control Plane:** Managed by Databricks in their cloud account. It contains the **Web UI**, **Notebook source code**, **Job scheduler**, and **Cluster management** services.\n",
    "* **Compute Plane:** Typically resides in *your* cloud account (AWS/Azure/GCP). This is where your data is actually processed by clusters of virtual machines, ensuring your data never leaves your security boundary.\n",
    "\n",
    "##### 2. Unity Catalog (The Governance Layer)\n",
    "\n",
    "* The modern \"brain\" of the workspace that manages fine-grained permissions.\n",
    "* It follows a **Three-Tier Namespace**: `Catalog` → `Schema (Database)` → `Table/Volume`.\n",
    "* This allows you to manage access to data across multiple workspaces from one central place.\n",
    "\n",
    "##### 3. Compute Resources (The Engine)\n",
    "\n",
    "* **All-Purpose Clusters:** Used for interactive analysis and development in notebooks.\n",
    "* **Job Clusters:** Transient clusters that spin up only for a specific automated task and shut down immediately after to save costs.\n",
    "* **SQL Warehouses:** Specialized compute optimized for running SQL queries and powering BI dashboards.\n",
    "\n",
    "##### 4. Workspace Assets (The \"Filesystem\")\n",
    "\n",
    "* **Notebooks:** Collaborative documents containing runnable code (Python, SQL, Scala, R), visualizations, and narrative text.\n",
    "* **Git Folders (Repos):** Integrated version control that lets you sync your Databricks notebooks directly with GitHub, GitLab, or Bitbucket.\n",
    "* **Dashboards:** Real-time data visualizations built directly on top of your processed tables.\n",
    "\n",
    "##### 5. Databricks SQL\n",
    "\n",
    "* A dedicated workspace persona for SQL analysts.\n",
    "* Includes a **Query Editor**, **Visualization tools**, and **Alerts** that notify you if a specific data metric (like \"daily revenue\") drops below a threshold.\n",
    "\n",
    "##### 6. Workflows & Orchestration\n",
    "\n",
    "* **Jobs:** The tool used to schedule notebooks or JARs to run at specific times or in response to events.\n",
    "* **Delta Live Tables (DLT):** A framework for building reliable, maintainable, and testable data processing pipelines.\n",
    "\n",
    "##### 7. Storage Foundation\n",
    "\n",
    "* **DBFS (Databricks File System):** A layer over your cloud storage (S3/ADLS) that makes it look like a local file system.\n",
    "* **Metastore:** The central repository for metadata (table definitions, partitions) so the compute knows how to read your raw files as structured tables.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8d5dc6a-cc34-487c-837e-3858f8741fe8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### **_4. Industry use cases (Netflix, Shell, Comcast)_**\n",
    "Databricks has become the go-to platform for massive enterprises because it handles the scale and real-time demands that traditional tools cannot.\n",
    "\n",
    "Here are the specific ways Netflix, Shell, and Comcast use the platform:\n",
    "\n",
    "#### _Netflix: Content & Experience Personalization_\n",
    "\n",
    "* **Recommendation Engine:** Processes billions of user interactions (viewing history, clicks, search patterns) to power the \"Top Picks\" and \"Because You Watched\" algorithms.\n",
    "* **Artwork Personalization:** Uses machine learning to choose which movie thumbnail to show you based on your taste (e.g., showing the lead actor if you like their work).\n",
    "* **Content Acquisition:** Analyzes global viewing trends to predict which new shows or movies are worth the multi-million dollar investment for production or licensing.\n",
    "* **A/B Testing at Scale:** Rapidly tests different UI layouts and features across millions of users simultaneously to see which drives the highest retention.\n",
    "\n",
    "#### *Shell: The Energy Transition & Reliability*\n",
    "\n",
    "* **Predictive Maintenance:** Monitors millions of high-frequency sensors on wind turbines, oil rigs, and solar panels to predict equipment failure before it happens.\n",
    "* **Carbon Reduction:** Analyzes data from across its global supply chain and refineries to optimize energy use and lower its overall carbon footprint.\n",
    "* **Renewable Energy Forecasting:** Combines weather data (wind speed, solar irradiance) with power grid demand to optimize how and when renewable energy is sold.\n",
    "* **Digital Twins:** Creates virtual replicas of physical assets (like an offshore rig) to simulate changes and optimize production in a safe, digital environment.\n",
    "\n",
    "#### *Comcast: Voice Commands & Customer Insight*\n",
    "\n",
    "* **Voice Remote Excellence:** Processes billions of voice commands from 20+ million remotes in real-time, allowing users to find content instantly using AI.\n",
    "* **Infrastructure Optimization:** Replaced 640 legacy machines with just 64 Databricks nodes, reducing compute costs by **10x** while increasing speed.\n",
    "* **Proactive Support:** Analyzes customer telemetry data to identify service issues or \"frustration signals\" before the customer even picks up the phone to complain.\n",
    "* **Real-time Ad Targeting:** Uses massive viewership datasets to help advertisers deliver relevant commercials to 125 million households across traditional and streaming TV.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "341d9096-b9de-49cf-b1be-bbd930d5e97c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Practice**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c244959f-20df-49e5-b4e0-d67d51c8baaf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n|product|price|\n+-------+-----+\n| iPhone|  999|\n|Samsung|  799|\n|MacBook| 1299|\n+-------+-----+\n\n+-------+-----+\n|product|price|\n+-------+-----+\n|MacBook| 1299|\n+-------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Create simple DataFrame\n",
    "data = [(\"iPhone\", 999), (\"Samsung\", 799), (\"MacBook\", 1299)]\n",
    "df = spark.createDataFrame(data, [\"product\", \"price\"])\n",
    "df.show()\n",
    "\n",
    "# Filter expensive products\n",
    "df.filter(df.price > 1000).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55c89923-0b99-4f4c-a087-90b024e5a449",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bef3d728-372f-47be-864f-66b9146190b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Resources**\n",
    "\n",
    "- [Databricks Trial](https://www.databricks.com/try-databricks)\n",
    "- [Databricks Quickstart](https://docs.databricks.com/en/introduction/)\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Day_1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}