{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e62dcd08-2680-437a-b10c-4c9b6b2f58d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# **PHASE 1: FOUNDATION (Days 1-4)**\n",
    "\n",
    "## **DAY 3 (11/01/26) - PySpark Transformations Deep Dive**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7532b03a-927e-4e8a-9856-33d66f08e2ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Section 1 - Learn**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e8b751c-3188-4f16-b99b-334bf092a600",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **_1. PySpark vs Pandas comparison_**\n",
    "\n",
    "Choosing between **PySpark** and **Pandas** usually comes down to the size of your data and the environment you are working in. While they look similar in syntax, their \"engines\" are fundamentally different.\n",
    "\n",
    "Here are the key comparison points:\n",
    "\n",
    "* **Single Machine vs. Distributed:** **Pandas** runs on a single node (your laptop or one server) and is limited by that machine's RAM. **PySpark** is distributed; it splits data into partitions and processes them across a cluster of many machines.\n",
    "* **Memory Management:** **Pandas** loads the entire dataset into memory at once—if your file is 20GB and your RAM is 16GB, it crashes. **PySpark** can handle datasets far larger than the memory of any single machine by \"spilling\" to disk or using the combined RAM of the whole cluster.\n",
    "* **Execution Strategy:** **Pandas** is **Eager**; it executes every command as soon as you run the cell. **PySpark** is **Lazy**; it builds a \"plan\" (DAG) and waits until you call an action (like `.show()` or `.save()`) before doing the actual work.\n",
    "* **Performance Bottlenecks:** For small datasets (under 1–2 GB), **Pandas** is often faster because it doesn't have the \"overhead\" of coordinating multiple machines. For large data, **PySpark** wins because it parallelizes the workload.\n",
    "* **Immutability:** **Pandas DataFrames** are mutable (you can change values in place). **PySpark DataFrames** are immutable; every transformation creates a *new* DataFrame, which is essential for tracking data lineage and recovering from errors.\n",
    "* **Ecosystem & Visualization:** **Pandas** has deep integration with the Python data science stack (Scikit-Learn, Matplotlib, Seaborn). **PySpark** requires extra steps (like converting a sample back to Pandas) to use these visualization tools easily.\n",
    "* **Fault Tolerance:** If a **Pandas** job fails halfway, you lose the progress in memory. **PySpark** is fault-tolerant; if one machine in the cluster fails, the \"Driver\" knows how to re-run only the missing tasks on a different machine.\n",
    "\n",
    "#### **Comparison Table**\n",
    "\n",
    "| Feature | Pandas | PySpark |\n",
    "| --- | --- | --- |\n",
    "| **Data Size** | Small/Medium (up to 5-10GB) | Big Data (TB to PB scale) |\n",
    "| **Parallelism** | Single-threaded | Multi-node parallelism |\n",
    "| **Optimization** | Minimal | High (Catalyst & Tungsten) |\n",
    "| **Learning Curve** | Low (Very intuitive) | Moderate (Requires Spark knowledge) |\n",
    "| **Streaming** | No native support | Yes (Structured Streaming) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06338551-afd0-41fe-b5d7-60242170a294",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d043ee4d-c347-477b-8152-f121b00d7b89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **_2. Joins (inner, left, right, outer)_**\n",
    "\n",
    "In PySpark, joins allow you to combine two DataFrames based on a common key. Because Spark is a distributed system, performing joins often involves a \"Shuffle,\" where data is moved across the network to ensure matching keys end up on the same machine.\n",
    "\n",
    "Here are the 5 essential join types in bullet points:\n",
    "\n",
    "* **Inner Join (Default):** The most restrictive join. It returns only the rows where there is a **matching key in both** the left and right DataFrames. If a key exists in one but not the other, it is dropped.\n",
    "* **Left Outer Join:** Keeps **all rows from the left** DataFrame. If there is no match in the right DataFrame, the resulting columns from the right side will contain `null`. This is the most common join for enriching a primary dataset.\n",
    "* **Right Outer Join:** The mirror of the Left Join. It keeps **all rows from the right** DataFrame. If no match is found in the left side, those columns will contain `null`. (Often avoided in favor of Left Joins for better readability).\n",
    "* **Full Outer Join:** The most inclusive join. It retains **all rows from both** DataFrames. It matches rows where keys exist in both and fills in `null` wherever a match is missing on either side.\n",
    "* **Left Semi Join:** A unique \"filtering\" join. It returns only rows from the left DataFrame that have a **match in the right** DataFrame. Unlike an Inner Join, it *only* keeps columns from the left side and does not create duplicate rows if multiple matches exist in the right.\n",
    "* **Left Anti Join:** The opposite of Semi Join. It returns only the rows from the left DataFrame that **do NOT have a match** in the right. It is extremely useful for finding discrepancies or \"missing\" records between two datasets.\n",
    "\n",
    "#### **PySpark Join Syntax**\n",
    "\n",
    "```python\n",
    "# Generic syntax\n",
    "df_joined = df_left.join(df_right, df_left.id == df_right.id, \"join_type\")\n",
    "\n",
    "```\n",
    "\n",
    "#### **Performance Tip: Broadcast Joins**\n",
    "\n",
    "When joining a **large table** with a **very small table**, Spark can perform a \"Broadcast Join.\" Instead of shuffling the big table across the network, it sends a copy of the small table to every machine. This significantly speeds up the process and reduces network overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7aab340-5657-4263-838b-2738d2005519",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc4ae1f6-7b52-4f7c-8abc-617065010a5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **_3. Window functions (running totals, rankings)_**\n",
    "\n",
    "In PySpark, **Window Functions** allow you to perform calculations across a specific \"window\" or group of rows related to the current row, without collapsing them into a single output row (unlike a `groupBy`).\n",
    "\n",
    "#### **The Core Components**\n",
    "\n",
    "To use a window function, you must define a **Window Specification** using three main parts:\n",
    "\n",
    "* **`partitionBy`**: Groups the rows (e.g., \"Group by Department\").\n",
    "* **`orderBy`**: Sorts the rows within that group (e.g., \"Sort by Salary\").\n",
    "* **`rowsBetween`**/**`rangeBetween`**: Defines the \"frame\" or boundaries (e.g., \"From the start of the group to the current row\").\n",
    "\n",
    "#### **Key Use Cases**\n",
    "\n",
    "* **Ranking Functions:**\n",
    "  * **`row_number()`**: Assigns a unique, sequential number to rows (1, 2, 3, 4).\n",
    "  * **`rank()`**: Assigns numbers but leaves gaps if there is a tie (1, 2, 2, 4).\n",
    "  * **`dense_rank()`**: Assigns numbers without leaving gaps for ties (1, 2, 2, 3).\n",
    "\n",
    "\n",
    "* **Running Totals (Cumulative Sums):**\n",
    "  * Uses `sum()` combined with a window that starts from the beginning of the partition up to the current row (`Window.unboundedPreceding, Window.currentRow`).\n",
    "\n",
    "\n",
    "* **Lead and Lag (Time-series analysis):**\n",
    "  * **`lag()`**: Fetches a value from a *previous* row (useful for calculating day-over-day growth).\n",
    "  * **`lead()`**: Fetches a value from a *subsequent* row (useful for seeing \"what happens next\").\n",
    "\n",
    "\n",
    "* **Moving Averages:**\n",
    "  * Calculates the average of a sliding window, such as the \"previous 7 days\" of sales data to smooth out fluctuations.\n",
    "\n",
    "\n",
    "* **Analytical Aggregates:**\n",
    "  * Unlike a standard `groupBy`, a Window `avg()` or `max()` appends the result as a new column to **every row** in the original dataset, allowing you to compare an individual's value against the group average.\n",
    "\n",
    "\n",
    "\n",
    "#### **PySpark Example: Running Total**\n",
    "\n",
    "```python\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# 1. Define the window\n",
    "windowSpec = Window.partitionBy(\"Department\").orderBy(\"Date\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "# 2. Apply the window function\n",
    "df.withColumn(\"Running_Total\", F.sum(\"Sales\").over(windowSpec)).show()\n",
    "```\n",
    "\n",
    "#### **Performance Note**\n",
    "\n",
    "**Avoid Empty** `partitionBy()`: If you don't define a partition, Spark is forced to move **all data** to a single machine (the Driver) to calculate the window. This will cause a \"bottleneck\" or an Out of Memory (OOM) error on large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64198984-5d5b-4a51-936b-a237fd10e5be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e6816f6-7d6f-4983-81a3-107551655a99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **_4. User-Defined Functions (UDFs)_**\n",
    "\n",
    "User-Defined Functions (UDFs) allow you to extend PySpark’s functionality by writing custom Python code to handle transformations that aren't possible with built-in functions.\n",
    "\n",
    "However, they come with a significant \"performance tax\" that every developer should understand:\n",
    "\n",
    "* **Custom Logic Extension:** Use UDFs only when Spark’s hundreds of built-in functions (like `when`, `substring`, or `date_add`) cannot satisfy your business logic.\n",
    "* **The Serialization Bottleneck:** Standard Python UDFs are slow because Spark must serialize each row, move it from the **JVM** (where Spark lives) to a **Python process**, execute your code, and then move the result back.\n",
    "* **\"Black Box\" Problem:** Spark’s **Catalyst Optimizer** cannot \"see\" inside a UDF. It can't optimize the code or perform \"predicate pushdown\" (filtering data early), which often leads to 10x slower execution compared to native functions.\n",
    "* **Pandas UDFs (Vectorized):** Introduced to solve the performance issue, **Pandas UDFs** use **Apache Arrow** to move data in \"batches\" rather than row-by-row. This significantly reduces overhead and can be up to 100x faster than standard UDFs.\n",
    "* **Type Hinting Requirement:** In newer Spark versions (3.0+), you should use Python type hints (e.g., `def func(s: pd.Series) -> pd.Series`) so Spark knows exactly how to handle the data types.\n",
    "* **Unity Catalog Governance:** In modern Databricks environments, you can register UDFs in the **Unity Catalog**, allowing them to be shared across the entire organization and governed with specific permissions.\n",
    "* **Best Practice Rule:** **Built-in Functions > Pandas UDFs > Standard UDFs.** Always search the `pyspark.sql.functions` module before writing a custom UDF.\n",
    "\n",
    "#### **Code Comparison: Standard vs. Pandas UDF**\n",
    "\n",
    "| Feature | Standard UDF (`@udf`) | Pandas UDF (`@pandas_udf`) |\n",
    "| --- | --- | --- |\n",
    "| **Data Handling** | Row-by-row | Batch-by-batch (Vectorized) |\n",
    "| **Performance** | Slow (High overhead) | Fast (Uses Apache Arrow) |\n",
    "| **Input Type** | Python primitives (int, str) | `pandas.Series` or `pandas.DataFrame` |\n",
    "| **Best For** | Simple, non-vectorizable logic | Statistical models, ML, complex math |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8addbe16-6bbb-4ca7-b2cc-0668ba62bca3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20f23c91-cbe8-4f2e-9e7e-9f255237199a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Practice**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9f15bb0-92a0-485b-bd7b-7bb97e6afcc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfb66933-8549-4f61-9ac8-8d34e5f25bc3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def load_ecommerce_dataset(Month_name):\n",
    "    df = spark.read.csv(f\"/Volumes/workspace/ecommerce/ecommerce_data/2019-{Month_name}.csv\", header=True, inferSchema=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c21086e-7ca8-4253-bb99-63e35b3ee023",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df_n = load_ecommerce_dataset(\"Nov\")\n",
    "df_o = load_ecommerce_dataset(\"Oct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d8e9a32-301b-44f8-9c25-e97c74c285cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### **1. Joins** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e15da70-2cd4-4db6-afeb-aa618ed6aaf5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creating a subset for purchases and views\n",
    "purchases = df_o.filter(df_o.event_type == \"purchase\").select(\n",
    "    \"user_session\", \"product_id\", F.col(\"event_time\").alias(\"purchase_time\")\n",
    ")\n",
    "\n",
    "views = df_o.filter(df_o.event_type == \"view\").select(\n",
    "    \"user_session\", \"product_id\", F.col(\"event_time\").alias(\"view_time\"), \"brand\", \"price\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8768fc00-c792-4144-988b-6298a079526c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Join to find views that resulted in a purchase in the same session for the same product\n",
    "conversion_df = views.join(\n",
    "    purchases, \n",
    "    on=[\"user_session\", \"product_id\"], \n",
    "    how=\"inner\"\n",
    ").filter(F.col(\"purchase_time\") >= F.col(\"view_time\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cd874f3-13c7-4953-bceb-064f7ac7d77f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>user_session</th><th>product_id</th><th>view_time</th><th>brand</th><th>price</th><th>purchase_time</th></tr></thead><tbody><tr><td>56fe5a0c-5744-4a2a-aa6d-d5b4ef99fe86</td><td>1002633</td><td>2019-10-01T04:53:26.000Z</td><td>apple</td><td>360.08</td><td>2019-10-01T04:59:25.000Z</td></tr><tr><td>56fe5a0c-5744-4a2a-aa6d-d5b4ef99fe86</td><td>1002633</td><td>2019-10-01T04:54:33.000Z</td><td>apple</td><td>360.08</td><td>2019-10-01T04:59:25.000Z</td></tr><tr><td>56fe5a0c-5744-4a2a-aa6d-d5b4ef99fe86</td><td>1002633</td><td>2019-10-01T04:58:42.000Z</td><td>apple</td><td>360.08</td><td>2019-10-01T04:59:25.000Z</td></tr><tr><td>646820dd-045d-4028-9929-75627384eaa9</td><td>1004250</td><td>2019-10-01T05:14:53.000Z</td><td>apple</td><td>804.21</td><td>2019-10-01T05:15:05.000Z</td></tr><tr><td>77b8992b-6844-48ac-8c45-e1dd2f53b567</td><td>21900034</td><td>2019-10-01T05:29:46.000Z</td><td>null</td><td>3.18</td><td>2019-10-01T05:33:11.000Z</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "56fe5a0c-5744-4a2a-aa6d-d5b4ef99fe86",
         1002633,
         "2019-10-01T04:53:26.000Z",
         "apple",
         360.08,
         "2019-10-01T04:59:25.000Z"
        ],
        [
         "56fe5a0c-5744-4a2a-aa6d-d5b4ef99fe86",
         1002633,
         "2019-10-01T04:54:33.000Z",
         "apple",
         360.08,
         "2019-10-01T04:59:25.000Z"
        ],
        [
         "56fe5a0c-5744-4a2a-aa6d-d5b4ef99fe86",
         1002633,
         "2019-10-01T04:58:42.000Z",
         "apple",
         360.08,
         "2019-10-01T04:59:25.000Z"
        ],
        [
         "646820dd-045d-4028-9929-75627384eaa9",
         1004250,
         "2019-10-01T05:14:53.000Z",
         "apple",
         804.21,
         "2019-10-01T05:15:05.000Z"
        ],
        [
         "77b8992b-6844-48ac-8c45-e1dd2f53b567",
         21900034,
         "2019-10-01T05:29:46.000Z",
         null,
         3.18,
         "2019-10-01T05:33:11.000Z"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "user_session",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "product_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "view_time",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "brand",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "price",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "purchase_time",
         "type": "\"timestamp\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(conversion_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3438530d-e61d-4fb2-bcb0-1cfae0e7a36e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### **2. Running Totals with Window Functions** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23b8c73b-4564-44f3-b51e-07bbb033e8e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4002f35-c222-4dc1-b308-406badf5372d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the Window: Group by category only (no ordering needed for simple averages)\n",
    "category_window = Window.partitionBy(\"category_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "210a40cf-c1c4-44cf-8abd-76739d06cb70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Apply the window function\n",
    "df_simple = df_o.withColumn(\n",
    "    \"avg_category_price\", \n",
    "    F.avg(\"price\").over(category_window)\n",
    ").withColumn(\n",
    "    \"price_diff\", \n",
    "    F.col(\"price\") - F.col(\"avg_category_price\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f17cf4f1-cc79-448d-9e05-d31efbe2bc16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>product_id</th><th>category_id</th><th>price</th><th>avg_category_price</th><th>price_diff</th></tr></thead><tbody><tr><td>8600174</td><td>2053013552226107603</td><td>10.27</td><td>63.36775169300209</td><td>-53.097751693002095</td></tr><tr><td>8600067</td><td>2053013552226107603</td><td>17.99</td><td>63.36775169300209</td><td>-45.3777516930021</td></tr><tr><td>8600067</td><td>2053013552226107603</td><td>17.99</td><td>63.36775169300209</td><td>-45.3777516930021</td></tr><tr><td>8600134</td><td>2053013552226107603</td><td>102.81</td><td>63.36775169300209</td><td>39.44224830699791</td></tr><tr><td>8600236</td><td>2053013552226107603</td><td>457.89</td><td>63.36775169300209</td><td>394.5222483069979</td></tr><tr><td>8600135</td><td>2053013552226107603</td><td>16.71</td><td>63.36775169300209</td><td>-46.65775169300209</td></tr><tr><td>8600235</td><td>2053013552226107603</td><td>463.73</td><td>63.36775169300209</td><td>400.3622483069979</td></tr><tr><td>8600094</td><td>2053013552226107603</td><td>69.47</td><td>63.36775169300209</td><td>6.102248306997907</td></tr><tr><td>8600204</td><td>2053013552226107603</td><td>30.96</td><td>63.36775169300209</td><td>-32.40775169300209</td></tr><tr><td>8600227</td><td>2053013552226107603</td><td>13.77</td><td>63.36775169300209</td><td>-49.597751693002095</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         8600174,
         2053013552226107603,
         10.27,
         63.36775169300209,
         -53.097751693002095
        ],
        [
         8600067,
         2053013552226107603,
         17.99,
         63.36775169300209,
         -45.3777516930021
        ],
        [
         8600067,
         2053013552226107603,
         17.99,
         63.36775169300209,
         -45.3777516930021
        ],
        [
         8600134,
         2053013552226107603,
         102.81,
         63.36775169300209,
         39.44224830699791
        ],
        [
         8600236,
         2053013552226107603,
         457.89,
         63.36775169300209,
         394.5222483069979
        ],
        [
         8600135,
         2053013552226107603,
         16.71,
         63.36775169300209,
         -46.65775169300209
        ],
        [
         8600235,
         2053013552226107603,
         463.73,
         63.36775169300209,
         400.3622483069979
        ],
        [
         8600094,
         2053013552226107603,
         69.47,
         63.36775169300209,
         6.102248306997907
        ],
        [
         8600204,
         2053013552226107603,
         30.96,
         63.36775169300209,
         -32.40775169300209
        ],
        [
         8600227,
         2053013552226107603,
         13.77,
         63.36775169300209,
         -49.597751693002095
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "product_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "category_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "price",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "avg_category_price",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "price_diff",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_simple.select(\"product_id\", \"category_id\", \"price\", \"avg_category_price\", \"price_diff\").head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2efc4de4-eb23-4d27-badc-16a95bded902",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### **3. Create derived features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6342bec-fc8f-4cb1-ac68-f25f4f27f440",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "user_profile = df_o.groupBy(\"user_id\").agg(\n",
    "    F.max(\"event_time\").alias(\"last_active\"),\n",
    "    F.avg(\"price\").alias(\"avg_interacted_price\"),\n",
    "    F.mode(\"brand\").alias(\"favorite_brand\") # Requires Spark 3.4+\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89130093-d809-460c-b630-d3c4b22914fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate Days since last activity\n",
    "current_date = df_o.select(F.max(\"event_time\")).collect()[0][0]\n",
    "user_profile = user_profile.withColumn(\n",
    "    \"days_since_active\", \n",
    "    F.datediff(F.lit(current_date), F.col(\"last_active\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74a7589d-f0b5-4f9e-ae3a-5dc18f7e435c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>user_id</th><th>last_active</th><th>avg_interacted_price</th><th>favorite_brand</th><th>days_since_active</th></tr></thead><tbody><tr><td>522453283</td><td>2019-10-01T00:11:44.000Z</td><td>136.2</td><td>samsung</td><td>30</td></tr><tr><td>554935393</td><td>2019-10-01T00:17:43.000Z</td><td>225.5388888888889</td><td>bosch</td><td>30</td></tr><tr><td>547718922</td><td>2019-10-21T12:15:31.000Z</td><td>292.96183333333335</td><td>lenovo</td><td>10</td></tr><tr><td>526949848</td><td>2019-10-21T15:54:20.000Z</td><td>290.73660377358493</td><td>luminarc</td><td>10</td></tr><tr><td>555467783</td><td>2019-10-01T02:58:12.000Z</td><td>15.42</td><td>samsung</td><td>30</td></tr><tr><td>548060580</td><td>2019-10-02T04:29:56.000Z</td><td>159.7057142857143</td><td>samsung</td><td>29</td></tr><tr><td>516407514</td><td>2019-10-15T05:10:19.000Z</td><td>224.0890909090909</td><td>vivo</td><td>16</td></tr><tr><td>513191866</td><td>2019-10-26T06:39:34.000Z</td><td>435.6627272727273</td><td>samsung</td><td>5</td></tr><tr><td>513386589</td><td>2019-10-29T06:07:56.000Z</td><td>250.3921739130435</td><td>samsung</td><td>2</td></tr><tr><td>512769367</td><td>2019-10-23T09:22:59.000Z</td><td>195.60457627118643</td><td>beko</td><td>8</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         522453283,
         "2019-10-01T00:11:44.000Z",
         136.2,
         "samsung",
         30
        ],
        [
         554935393,
         "2019-10-01T00:17:43.000Z",
         225.5388888888889,
         "bosch",
         30
        ],
        [
         547718922,
         "2019-10-21T12:15:31.000Z",
         292.96183333333335,
         "lenovo",
         10
        ],
        [
         526949848,
         "2019-10-21T15:54:20.000Z",
         290.73660377358493,
         "luminarc",
         10
        ],
        [
         555467783,
         "2019-10-01T02:58:12.000Z",
         15.42,
         "samsung",
         30
        ],
        [
         548060580,
         "2019-10-02T04:29:56.000Z",
         159.7057142857143,
         "samsung",
         29
        ],
        [
         516407514,
         "2019-10-15T05:10:19.000Z",
         224.0890909090909,
         "vivo",
         16
        ],
        [
         513191866,
         "2019-10-26T06:39:34.000Z",
         435.6627272727273,
         "samsung",
         5
        ],
        [
         513386589,
         "2019-10-29T06:07:56.000Z",
         250.3921739130435,
         "samsung",
         2
        ],
        [
         512769367,
         "2019-10-23T09:22:59.000Z",
         195.60457627118643,
         "beko",
         8
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "user_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "last_active",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "avg_interacted_price",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "favorite_brand",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "days_since_active",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(user_profile.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d50dc071-2109-49a7-a91a-ce54a72c564d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49b7e531-86e6-497b-a178-6bf8cd8f1af9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Resources**\n",
    "- [pyspark official docs](https://docs.databricks.com/pyspark/)\n",
    "- [pySpark window Functions](https://medium.com/@uzzaman.ahmed/pyspark-window-functions-a-comprehensive-guide-dc9bdad8c7ae)\n",
    "- [pySpark UDFs](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.udf.html)\n",
    "- [pySpark joins](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.join.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e6eb3fc-5a0b-4de3-a400-fcc80f5b2d9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "----"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Day_3",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}