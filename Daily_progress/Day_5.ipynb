{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e62dcd08-2680-437a-b10c-4c9b6b2f58d9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 1"
    }
   },
   "source": [
    "# **PHASE 2: DATA ENGINEERING (Days 5-8)**\n",
    "\n",
    "## **DAY 5 (13/01/26) - Delta Lake Advanced**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7532b03a-927e-4e8a-9856-33d66f08e2ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Section 1 - Learn**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e8b751c-3188-4f16-b99b-334bf092a600",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **_1. Time travel (version history)_**\n",
    "\n",
    "Delta Lake **Time Travel** is a powerful feature that allows you to query historical versions of your data. Because every transaction (Insert, Update, Delete) is recorded in a versioned **Transaction Log**, you can \"go back in time\" to see exactly what your table looked like at any specific point.\n",
    "\n",
    "Here are the key points in bullet form:\n",
    "\n",
    "#### 1. How to Access History\n",
    "\n",
    "* **The \"DVR\" for Data:** Every write operation creates a new **Version Number** (starting at 0) and a **Timestamp**.\n",
    "* **Version Query:** You can query a specific version using `VERSION AS OF X`.\n",
    "* **Timestamp Query:** You can query by a date or time string using `TIMESTAMP AS OF 'yyyy-MM-dd'`.\n",
    "\n",
    "#### 2. Core Use Cases\n",
    "\n",
    "* **Audit & Compliance:** Easily prove what data looked like during a specific month-end or financial close without maintaining manual backups.\n",
    "* **Error Recovery:** If a bug in your code accidentally deletes or corrupts rows, you can instantly see the \"good\" data from the previous version.\n",
    "* **ML Reproducibility:** Data scientists can re-run models on the exact same dataset version used months ago, ensuring consistent results for experiments.\n",
    "* **Snapshot Isolation:** You can \"pin\" a set of queries to a single version of a fast-changing table so that your multi-step report doesn't change halfway through execution.\n",
    "\n",
    "#### 3. Key Commands (PySpark & SQL)\n",
    "\n",
    "* **View Table History:** Use `DESCRIBE HISTORY table_name` to see a list of versions, who made the change, and what operation was performed (e.g., MERGE, UPDATE).\n",
    "* **Rollback (Restore):** You can permanently revert a table to a previous state using the `RESTORE` command:\n",
    "  * *SQL:* `RESTORE TABLE my_table TO VERSION AS OF 5`\n",
    "  * *Python:* `deltaTable.restoreToVersion(5)`\n",
    "\n",
    "\n",
    "\n",
    "#### 4. Important Constraints\n",
    "\n",
    "* **The \"Vacuum\" Limit:** You can only time travel to versions for which the underlying data files still exist. If you run the **`VACUUM`** command (which deletes old files to save space), you lose the ability to go back past that cleanup point.\n",
    "* **Default Retention:** By default, Databricks keeps 30 days of transaction logs and 7 days of actual data files for time travel. You can increase these durations in the table properties:\n",
    "  * `delta.logRetentionDuration` (History log)\n",
    "  * `delta.deletedFileRetentionDuration` (Physical data files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06338551-afd0-41fe-b5d7-60242170a294",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d043ee4d-c347-477b-8152-f121b00d7b89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **_2. MERGE operations (upserts)_**\n",
    "\n",
    "In PySpark, the **`MERGE`** operation (often called an **Upsert**) is a single, powerful command that allows you to simultaneously insert new records, update existing ones, and even delete data based on a matching condition.\n",
    "\n",
    "In a traditional Data Lake (Parquet), you would have to rewrite the entire partition to change a single row. Delta Lake makes this efficient by only rewriting the specific files that contain the \"touched\" data.\n",
    "\n",
    "#### **The Three Core Clauses**\n",
    "\n",
    "* **`whenMatchedUpdate(...)`**: If the `source.id` matches a `target.id`, update the existing row in the target table with new values.\n",
    "* **`whenNotMatchedInsert(...)`**: If a record exists in the source but **not** in the target, insert it as a brand-new row.\n",
    "* **`whenNotMatchedBySourceDelete()`**: (Optional) If a record exists in the target but is **missing** from the source, delete it from the target. This is useful for synchronizing a mirror image of a source database.\n",
    "\n",
    "#### **Key Industry Use Cases**\n",
    "\n",
    "* **Change Data Capture (CDC):** Syncing your Data Lake with a production SQL database by applying a daily stream of \"Inserts\" and \"Updates.\"\n",
    "* **GDPR Compliance:** Efficiently finding and deleting specific user records across massive datasets without a full table rewrite.\n",
    "* **SCD Type 1 & 2:** Maintaining \"Slowly Changing Dimensions\"—either overwriting old values (Type 1) or keeping a history of changes (Type 2).\n",
    "* **Deduplication:** Merging a new batch of data while ensuring you don't create duplicate records for users already in the system.\n",
    "\n",
    "#### **PySpark Code Example**\n",
    "\n",
    "```python\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# 1. Load the existing target Delta table\n",
    "target_table = DeltaTable.forPath(spark, \"/mnt/delta/customers\")\n",
    "\n",
    "# 2. Perform the Merge\n",
    "target_table.alias(\"t\").merge(\n",
    "    source = df_updates.alias(\"s\"),\n",
    "    condition = \"t.customer_id = s.customer_id\"\n",
    ").whenMatchedUpdate(set = {\n",
    "    \"email\": \"s.email\",\n",
    "    \"last_updated\": \"current_timestamp()\"\n",
    "}).whenNotMatchedInsert(values = {\n",
    "    \"customer_id\": \"s.customer_id\",\n",
    "    \"name\": \"s.name\",\n",
    "    \"email\": \"s.email\",\n",
    "    \"last_updated\": \"current_timestamp()\"\n",
    "}).execute()\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "#### **Performance Note: \"Shuffle\" Costs**\n",
    "\n",
    "A `MERGE` operation is essentially a **Join** followed by a **Write**. To keep it fast:\n",
    "\n",
    "* **Z-Order your join keys:** If you frequently merge on `customer_id`, Z-Ordering the target table by that column will help Spark find the relevant files much faster.\n",
    "* **Broadcast Small Sources:** If your \"updates\" DataFrame is small, Spark can broadcast it to all nodes, avoiding a massive shuffle of the multi-terabyte target table.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7aab340-5657-4263-838b-2738d2005519",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc4ae1f6-7b52-4f7c-8abc-617065010a5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **_3. OPTIMIZE & ZORDER_**\n",
    "\n",
    "In Delta Lake, `OPTIMIZE` and `ZORDER` are the two primary tools used to physically reorganize data on disk to improve query performance and reduce storage overhead.\n",
    "\n",
    "##### **1. OPTIMIZE (File Compaction)**\n",
    "\n",
    "* **The \"Small File Problem\":** Frequent streaming or small batch writes often create thousands of tiny Parquet files. This slows down queries because the engine spends more time opening/closing files and reading metadata than actually processing data.\n",
    "* **Bin-Packing:** The `OPTIMIZE` command performs \"compaction.\" It takes those small files and merges them into larger, right-sized files (defaulting to **1GB**).\n",
    "* **Metadata Efficiency:** By reducing the number of files, Spark has much less work to do when scanning the transaction log and listing files in cloud storage.\n",
    "\n",
    "##### **2. ZORDER (Data Clustering)**\n",
    "\n",
    "* **Multi-Dimensional Sorting:** While `OPTIMIZE` just makes files bigger, adding `ZORDER BY` physically **rearranges the rows** within those files based on specific columns.\n",
    "* **Data Skipping:** It co-locates related information. If you Z-Order by `customer_id`, rows for the same customer will be grouped into the same file. When you filter for that ID, Delta Lake skips the files that don't contain it by checking the min/max statistics in the log.\n",
    "* **High Cardinality Columns:** Z-Ordering is most effective on columns that have many unique values (like `ID`, `email`, or `phone_number`) and are frequently used in `WHERE` clauses or as `JOIN` keys.\n",
    "* **The 4-Column Limit:** Effectiveness drops if you Z-Order by too many columns (usually stay under 1–4). It is a \"trade-off\" tool—the more columns you Z-Order, the less \"tight\" the clustering becomes for each one.\n",
    "\n",
    "##### **Summary Comparison**\n",
    "\n",
    "| Feature | OPTIMIZE | ZORDER |\n",
    "| --- | --- | --- |\n",
    "| **Primary Goal** | Solve the \"Small File Problem\" | Enable advanced \"Data Skipping\" |\n",
    "| **What it does** | Merges small files into ~1GB files | Clusters related row values together |\n",
    "| **When to run** | After many small writes/updates | On large tables with frequent filters |\n",
    "| **Resource Cost** | Low to Moderate (I/O heavy) | High (Compute intensive sorting) |\n",
    "\n",
    "##### **PySpark & SQL Usage**\n",
    "\n",
    "```sql\n",
    "-- SQL Syntax\n",
    "OPTIMIZE my_table \n",
    "ZORDER BY (customer_id, event_type);\n",
    "\n",
    "```\n",
    "\n",
    "```python\n",
    "# PySpark Syntax\n",
    "from delta.tables import DeltaTable\n",
    "deltaTable = DeltaTable.forPath(spark, \"/path/to/table\")\n",
    "deltaTable.optimize().zorderBy(\"customer_id\").executeZOrderBy()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64198984-5d5b-4a51-936b-a237fd10e5be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e6816f6-7d6f-4983-81a3-107551655a99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **_4. VACUUM for cleanup_**\n",
    "\n",
    "In Delta Lake, the `VACUUM` command is the primary maintenance tool used to delete data files that are no longer needed by the current version of a table. It is essential for managing storage costs and maintaining data privacy (e.g., GDPR compliance).\n",
    "\n",
    "##### **1. Why do we need VACUUM?**\n",
    "\n",
    "* **Managing Deleted Files:** When you `UPDATE` or `DELETE` data in Delta Lake, the old files aren't physically erased immediately. They are simply \"marked as removed\" in the Transaction Log so that **Time Travel** is still possible.\n",
    "* **Storage Optimization:** Over time, these obsolete files accumulate, leading to \"orphaned\" data that takes up space and increases cloud storage costs. `VACUUM` permanently removes these files from the underlying storage (S3/ADLS/GCS).\n",
    "\n",
    "##### **2. The Safety Threshold (Retention Period)**\n",
    "\n",
    "* **Default Protection:** By default, `VACUUM` only deletes files that were deleted/overwritten **more than 7 days ago**.\n",
    "* **Preventing Corruption:** This 7-day buffer is a safety mechanism. If you vacuum files that are currently being read by a long-running Spark job, that job will fail because its source files suddenly vanished.\n",
    "* **Overriding Safety:** You can change this period (e.g., `VACUUM table RETAIN 24 HOURS`), but setting it to 0 is generally blocked unless you change a specific Spark configuration, as it risks corrupting active sessions.\n",
    "\n",
    "##### **3. Impact on Time Travel**\n",
    "\n",
    "* **Breaking the \"History\":** Once you run `VACUUM`, you lose the ability to **Time Travel** back to any version that relied on those deleted files.\n",
    "* **Metadata vs. Data:** The Transaction Log (the \"history list\") stays intact, but if you try to query an old version, you will get a `FileNotFoundException` because the physical data is gone.\n",
    "\n",
    "##### **4. Best Practices**\n",
    "\n",
    "* **Run Regularly:** Incorporate `VACUUM` into your weekly maintenance pipelines for tables with high update/delete frequency.\n",
    "* **Run AFTER Optimize:** It is common to run `OPTIMIZE` (which creates new large files) followed by `VACUUM` (which cleans up the old small files left behind).\n",
    "* **Dry Run First:** Always use the `DRY RUN` clause first to see exactly how many files and how much data will be deleted before actually committing to the cleanup.\n",
    "\n",
    "\n",
    "##### **Syntax Examples**\n",
    "\n",
    "**SQL:**\n",
    "\n",
    "```sql\n",
    "-- Preview what will be deleted\n",
    "VACUUM customers_table DRY RUN;\n",
    "\n",
    "-- Delete files older than the default 7 days\n",
    "VACUUM customers_table;\n",
    "\n",
    "-- Delete files older than 100 hours\n",
    "VACUUM customers_table RETAIN 100 HOURS;\n",
    "\n",
    "```\n",
    "\n",
    "**PySpark:**\n",
    "\n",
    "```python\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "deltaTable = DeltaTable.forPath(spark, \"/mnt/delta/customers\")\n",
    "deltaTable.vacuum(168) # 168 hours = 7 days\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8addbe16-6bbb-4ca7-b2cc-0668ba62bca3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20f23c91-cbe8-4f2e-9e7e-9f255237199a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Practice**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9f15bb0-92a0-485b-bd7b-7bb97e6afcc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfb66933-8549-4f61-9ac8-8d34e5f25bc3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def load_ecommerce_dataset(Month_name):\n",
    "    df = spark.read.csv(f\"/Volumes/workspace/ecommerce/ecommerce_data/2019-{Month_name}.csv\", header=True, inferSchema=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c21086e-7ca8-4253-bb99-63e35b3ee023",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df_n = load_ecommerce_dataset(\"Nov\")\n",
    "df_o = load_ecommerce_dataset(\"Oct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d8e9a32-301b-44f8-9c25-e97c74c285cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### **1. Implement incremental MERGE** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e15da70-2cd4-4db6-afeb-aa618ed6aaf5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, DoubleType, TimestampType\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd6cd094-49e3-4c11-98a5-ce494119752a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Row(event_time=datetime.datetime(2019, 10, 1, 0, 0, 1), event_type='view', product_id=1004856, category_id=2053013555631882655, category_code='electronics.smartphone', brand='samsung', price=130.25, user_id=541312840, user_session='72e0966e-bc31-40ef-9c63-907d2d5bcc89')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"event_time\", TimestampType(), True),\n",
    "    StructField(\"event_type\", StringType(), True),\n",
    "    StructField(\"product_id\", LongType(), True),\n",
    "    StructField(\"category_id\", LongType(), True),\n",
    "    StructField(\"category_code\", StringType(), True),\n",
    "    StructField(\"brand\", StringType(), True),\n",
    "    StructField(\"price\", DoubleType(), True),\n",
    "    StructField(\"user_id\", LongType(), True),\n",
    "    StructField(\"user_session\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create sample rows\n",
    "new_rows = [\n",
    "    # Row A: An update to an existing record (e.g., price change for a specific event)\n",
    "    Row(datetime.datetime(2019, 10, 1, 0, 0, 1), \"view\", 1004856, 2053013555631882655, \"electronics.smartphone\", \"samsung\", 130.25, 541312840, \"72e0966e-bc31-40ef-9c63-907d2d5bcc89\"),\n",
    "    \n",
    "    # Row B: A brand new event\n",
    "    Row(datetime.datetime(2026, 1, 13, 10, 0, 0), \"purchase\", 5000123, 2053013555631882655, \"electronics.audio\", \"apple\", 199.99, 999999999, \"new-session-uuid-123\")\n",
    "]\n",
    "\n",
    "# 3. Create the DataFrame\n",
    "updates_df = spark.createDataFrame(new_rows, schema=schema)\n",
    "\n",
    "display(updates_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98354983-9972-43cd-8f42-821797b831da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge operation completed successfully.\n"
     ]
    }
   ],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "target_table = DeltaTable.forName(spark, \"df_o_table\")\n",
    "\n",
    "target_table.alias(\"target\").merge(\n",
    "    updates_df.alias(\"source\"),\n",
    "    \"\"\"\n",
    "    target.event_time = source.event_time AND \n",
    "    target.user_id = source.user_id AND \n",
    "    target.product_id = source.product_id AND \n",
    "    target.event_type = source.event_type\n",
    "    \"\"\"\n",
    ") \\\n",
    ".whenMatchedUpdateAll() \\\n",
    ".whenNotMatchedInsertAll() \\\n",
    ".execute()\n",
    "\n",
    "print(\"Merge operation completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3438530d-e61d-4fb2-bcb0-1cfae0e7a36e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### **2. Query historical versions** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23b8c73b-4564-44f3-b51e-07bbb033e8e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>version</th><th>timestamp</th><th>userId</th><th>userName</th><th>operation</th><th>operationParameters</th><th>job</th><th>notebook</th><th>clusterId</th><th>readVersion</th><th>isolationLevel</th><th>isBlindAppend</th><th>operationMetrics</th><th>userMetadata</th><th>engineInfo</th></tr></thead><tbody><tr><td>3</td><td>2026-01-13T10:10:18.000Z</td><td>4346170070925815</td><td>shivamdubey2310711@gmail.com</td><td>MERGE</td><td>Map(predicate -> [\"(((event_time#13286 = event_time#13257) AND (cast(user_id#13293 as bigint) = user_id#13264L)) AND ((cast(product_id#13288 as bigint) = product_id#13259L) AND (event_type#13287 = event_type#13258)))\"], clusterBy -> [], matchedPredicates -> [{\"actionType\":\"update\"}], statsOnLoad -> true, notMatchedBySourcePredicates -> [], notMatchedPredicates -> [{\"actionType\":\"insert\"}])</td><td>null</td><td>List(2870774749036939)</td><td>0113-095357-ntupxiav-v2n</td><td>2</td><td>WriteSerializable</td><td>false</td><td>Map(numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 2, numTargetBytesAdded -> 4987, numTargetBytesRemoved -> 0, numTargetDeletionVectorsAdded -> 0, numTargetRowsMatchedUpdated -> 0, executionTimeMs -> 5224, materializeSourceTimeMs -> 215, numTargetRowsInserted -> 2, numTargetRowsMatchedDeleted -> 0, numTargetDeletionVectorsUpdated -> 0, scanTimeMs -> 3007, numTargetRowsUpdated -> 0, numOutputRows -> 2, numTargetDeletionVectorsRemoved -> 0, numTargetRowsNotMatchedBySourceUpdated -> 0, numTargetChangeFilesAdded -> 0, numSourceRows -> 2, numTargetFilesRemoved -> 0, numTargetRowsNotMatchedBySourceDeleted -> 0, rewriteTimeMs -> 1866)</td><td>null</td><td>Databricks-Runtime/17.3.x-aarch64-photon-scala2.13</td></tr><tr><td>2</td><td>2026-01-12T08:33:57.000Z</td><td>4346170070925815</td><td>shivamdubey2310711@gmail.com</td><td>CREATE OR REPLACE TABLE AS SELECT</td><td>Map(partitionBy -> [], clusterBy -> [], description -> null, isManaged -> true, properties -> {\"delta.enableDeletionVectors\":\"true\"}, statsOnLoad -> true)</td><td>null</td><td>List(3926879644577736)</td><td>0112-081808-3dvwp4c-v2n</td><td>1</td><td>WriteSerializable</td><td>false</td><td>Map(numFiles -> 43, numRemovedFiles -> 43, numRemovedBytes -> 1405244735, numDeletionVectorsRemoved -> 0, numOutputRows -> 42448764, numOutputBytes -> 1405244735)</td><td>null</td><td>Databricks-Runtime/17.3.x-aarch64-photon-scala2.13</td></tr><tr><td>1</td><td>2026-01-12T08:32:35.000Z</td><td>4346170070925815</td><td>shivamdubey2310711@gmail.com</td><td>CREATE OR REPLACE TABLE AS SELECT</td><td>Map(partitionBy -> [], clusterBy -> [], description -> null, isManaged -> true, properties -> {\"delta.enableDeletionVectors\":\"true\"}, statsOnLoad -> true)</td><td>null</td><td>List(3926879644577736)</td><td>0112-081808-3dvwp4c-v2n</td><td>0</td><td>WriteSerializable</td><td>false</td><td>Map(numFiles -> 43, numRemovedFiles -> 43, numRemovedBytes -> 1405244735, numDeletionVectorsRemoved -> 0, numOutputRows -> 42448764, numOutputBytes -> 1405244735)</td><td>null</td><td>Databricks-Runtime/17.3.x-aarch64-photon-scala2.13</td></tr><tr><td>0</td><td>2026-01-12T08:27:06.000Z</td><td>4346170070925815</td><td>shivamdubey2310711@gmail.com</td><td>CREATE OR REPLACE TABLE AS SELECT</td><td>Map(partitionBy -> [], clusterBy -> [], description -> null, isManaged -> true, properties -> {\"delta.enableDeletionVectors\":\"true\"}, statsOnLoad -> true)</td><td>null</td><td>List(3926879644577736)</td><td>0112-081808-3dvwp4c-v2n</td><td>null</td><td>WriteSerializable</td><td>false</td><td>Map(numFiles -> 43, numRemovedFiles -> 0, numRemovedBytes -> 0, numDeletionVectorsRemoved -> 0, numOutputRows -> 42448764, numOutputBytes -> 1405244735)</td><td>null</td><td>Databricks-Runtime/17.3.x-aarch64-photon-scala2.13</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         3,
         "2026-01-13T10:10:18.000Z",
         "4346170070925815",
         "shivamdubey2310711@gmail.com",
         "MERGE",
         {
          "clusterBy": "[]",
          "matchedPredicates": "[{\"actionType\":\"update\"}]",
          "notMatchedBySourcePredicates": "[]",
          "notMatchedPredicates": "[{\"actionType\":\"insert\"}]",
          "predicate": "[\"(((event_time#13286 = event_time#13257) AND (cast(user_id#13293 as bigint) = user_id#13264L)) AND ((cast(product_id#13288 as bigint) = product_id#13259L) AND (event_type#13287 = event_type#13258)))\"]",
          "statsOnLoad": "true"
         },
         null,
         [
          "2870774749036939"
         ],
         "0113-095357-ntupxiav-v2n",
         2,
         "WriteSerializable",
         false,
         {
          "executionTimeMs": "5224",
          "materializeSourceTimeMs": "215",
          "numOutputRows": "2",
          "numSourceRows": "2",
          "numTargetBytesAdded": "4987",
          "numTargetBytesRemoved": "0",
          "numTargetChangeFilesAdded": "0",
          "numTargetDeletionVectorsAdded": "0",
          "numTargetDeletionVectorsRemoved": "0",
          "numTargetDeletionVectorsUpdated": "0",
          "numTargetFilesAdded": "2",
          "numTargetFilesRemoved": "0",
          "numTargetRowsCopied": "0",
          "numTargetRowsDeleted": "0",
          "numTargetRowsInserted": "2",
          "numTargetRowsMatchedDeleted": "0",
          "numTargetRowsMatchedUpdated": "0",
          "numTargetRowsNotMatchedBySourceDeleted": "0",
          "numTargetRowsNotMatchedBySourceUpdated": "0",
          "numTargetRowsUpdated": "0",
          "rewriteTimeMs": "1866",
          "scanTimeMs": "3007"
         },
         null,
         "Databricks-Runtime/17.3.x-aarch64-photon-scala2.13"
        ],
        [
         2,
         "2026-01-12T08:33:57.000Z",
         "4346170070925815",
         "shivamdubey2310711@gmail.com",
         "CREATE OR REPLACE TABLE AS SELECT",
         {
          "clusterBy": "[]",
          "description": null,
          "isManaged": "true",
          "partitionBy": "[]",
          "properties": "{\"delta.enableDeletionVectors\":\"true\"}",
          "statsOnLoad": "true"
         },
         null,
         [
          "3926879644577736"
         ],
         "0112-081808-3dvwp4c-v2n",
         1,
         "WriteSerializable",
         false,
         {
          "numDeletionVectorsRemoved": "0",
          "numFiles": "43",
          "numOutputBytes": "1405244735",
          "numOutputRows": "42448764",
          "numRemovedBytes": "1405244735",
          "numRemovedFiles": "43"
         },
         null,
         "Databricks-Runtime/17.3.x-aarch64-photon-scala2.13"
        ],
        [
         1,
         "2026-01-12T08:32:35.000Z",
         "4346170070925815",
         "shivamdubey2310711@gmail.com",
         "CREATE OR REPLACE TABLE AS SELECT",
         {
          "clusterBy": "[]",
          "description": null,
          "isManaged": "true",
          "partitionBy": "[]",
          "properties": "{\"delta.enableDeletionVectors\":\"true\"}",
          "statsOnLoad": "true"
         },
         null,
         [
          "3926879644577736"
         ],
         "0112-081808-3dvwp4c-v2n",
         0,
         "WriteSerializable",
         false,
         {
          "numDeletionVectorsRemoved": "0",
          "numFiles": "43",
          "numOutputBytes": "1405244735",
          "numOutputRows": "42448764",
          "numRemovedBytes": "1405244735",
          "numRemovedFiles": "43"
         },
         null,
         "Databricks-Runtime/17.3.x-aarch64-photon-scala2.13"
        ],
        [
         0,
         "2026-01-12T08:27:06.000Z",
         "4346170070925815",
         "shivamdubey2310711@gmail.com",
         "CREATE OR REPLACE TABLE AS SELECT",
         {
          "clusterBy": "[]",
          "description": null,
          "isManaged": "true",
          "partitionBy": "[]",
          "properties": "{\"delta.enableDeletionVectors\":\"true\"}",
          "statsOnLoad": "true"
         },
         null,
         [
          "3926879644577736"
         ],
         "0112-081808-3dvwp4c-v2n",
         null,
         "WriteSerializable",
         false,
         {
          "numDeletionVectorsRemoved": "0",
          "numFiles": "43",
          "numOutputBytes": "1405244735",
          "numOutputRows": "42448764",
          "numRemovedBytes": "0",
          "numRemovedFiles": "0"
         },
         null,
         "Databricks-Runtime/17.3.x-aarch64-photon-scala2.13"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "version",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "timestamp",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "userId",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "userName",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "operation",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "operationParameters",
         "type": "{\"keyType\":\"string\",\"type\":\"map\",\"valueContainsNull\":true,\"valueType\":\"string\"}"
        },
        {
         "metadata": "{}",
         "name": "job",
         "type": "{\"fields\":[{\"metadata\":{},\"name\":\"jobId\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"jobName\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"jobRunId\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"runId\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"jobOwnerId\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"triggerType\",\"nullable\":true,\"type\":\"string\"}],\"type\":\"struct\"}"
        },
        {
         "metadata": "{}",
         "name": "notebook",
         "type": "{\"fields\":[{\"metadata\":{},\"name\":\"notebookId\",\"nullable\":true,\"type\":\"string\"}],\"type\":\"struct\"}"
        },
        {
         "metadata": "{}",
         "name": "clusterId",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "readVersion",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "isolationLevel",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "isBlindAppend",
         "type": "\"boolean\""
        },
        {
         "metadata": "{}",
         "name": "operationMetrics",
         "type": "{\"keyType\":\"string\",\"type\":\"map\",\"valueContainsNull\":true,\"valueType\":\"string\"}"
        },
        {
         "metadata": "{}",
         "name": "userMetadata",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "engineInfo",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "deltaTable = DeltaTable.forName(spark, \"df_o_table\")\n",
    "display(deltaTable.history())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9bbd65b-9995-4309-8b93-a7358b667d05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version 0 Count: 42448764\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Row(event_time=datetime.datetime(2019, 10, 19, 6, 15, 6), event_type='view', product_id=49300013, category_id=2125931803410694331, category_code=None, brand='weekend', price=875.18, user_id=515837919, user_session='0df7d269-da26-4087-b593-faee853ab0b5')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the table as it existed at version 0\n",
    "df_v0 = spark.read.format(\"delta\") \\\n",
    "    .option(\"versionAsOf\", 0) \\\n",
    "    .table(\"df_o_table\")\n",
    "\n",
    "print(f\"Version 0 Count: {df_v0.count()}\")\n",
    "display(df_v0.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2efc4de4-eb23-4d27-badc-16a95bded902",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### **3. Optimize**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6342bec-fc8f-4cb1-ac68-f25f4f27f440",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[path: string, metrics: struct<numFilesAdded:bigint,numFilesRemoved:bigint,filesAdded:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,filesRemoved:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,partitionsOptimized:bigint,zOrderStats:struct<strategyName:string,inputCubeFiles:struct<num:bigint,size:bigint>,inputOtherFiles:struct<num:bigint,size:bigint>,inputNumCubes:bigint,mergedFiles:struct<num:bigint,size:bigint>,numOutputCubes:bigint,mergedNumCubes:bigint>,clusteringStats:struct<inputZCubeFiles:struct<numFiles:bigint,size:bigint>,inputOtherFiles:struct<numFiles:bigint,size:bigint>,inputNumZCubes:bigint,mergedFiles:struct<numFiles:bigint,size:bigint>,numOutputZCubes:bigint>,numBins:bigint,numBatches:bigint,totalConsideredFiles:bigint,totalFilesSkipped:bigint,preserveInsertionOrder:boolean,numFilesSkippedToReduceWriteAmplification:bigint,numBytesSkippedToReduceWriteAmplification:bigint,startTimeMs:bigint,endTimeMs:bigint,totalClusterParallelism:bigint,totalScheduledTasks:bigint,autoCompactParallelismStats:struct<maxClusterActiveParallelism:bigint,minClusterActiveParallelism:bigint,maxSessionActiveParallelism:bigint,minSessionActiveParallelism:bigint>,deletionVectorStats:struct<numDeletionVectorsRemoved:bigint,numDeletionVectorRowsRemoved:bigint>,recompressionCodec:string,numTableColumns:bigint,numTableColumnsWithStats:bigint,totalTaskExecutionTimeMs:bigint,skippedArchivedFiles:bigint,clusteringMetrics:struct<sizeOfTableInBytesBeforeLazyClustering:bigint,isNewMetadataCreated:boolean,isPOTriggered:boolean,isFull:boolean,approxClusteringQuality:double,approxClusteringQualityPerColumn:array<double>,approxClusteringCoverage:double,compactionType:string,numFilesSkippedWithoutStats:bigint,numFilesClassifiedToIntermediateNodes:bigint,sizeOfFilesClassifiedToIntermediateNodesInBytes:bigint,logicalSizeOfFilesClassifiedToIntermediateNodesInBytes:bigint,numFilesClassifiedToLeafNodes:bigint,sizeOfFilesClassifiedToLeafNodesInBytes:bigint,logicalSizeOfFilesClassifiedToLeafNodesInBytes:bigint,numThreadsForClassifier:int,clusterThresholdStrategy:string,minFileSize:bigint,maxFileSize:bigint,nodeMinNumFilesToCompact:bigint,numIdealFiles:bigint,numIdealFilesWithTrimmedStringMaxValue:bigint,numAddedFilesWithSameMinMaxOnClusteringColumns:array<bigint>,numClusteringTasksPlanned:int,numClusteringTasksNotPlannedDueToPO:int,numCompactionTasksPlanned:int,numCompactionTasksPlannedUndoneDueToPO:int,numOptimizeBatchesPlanned:int,numLeafNodesExpanded:bigint,numLeafNodesClustered:bigint,numGetFilesForNodeCalls:bigint,numSamplingJobs:bigint,numLeafNodesCompacted:bigint,numLeafNodesCompactedUndoneDueToPO:bigint,numIntermediateNodesCompacted:bigint,numIntermediateNodesCompactedUndoneDueToPO:bigint,totalSizeOfDataToCompactInBytes:bigint,totalSizeOfDataToCompactInBytesUndoneDueToPO:bigint,totalLogicalSizeOfDataToCompactInBytes:bigint,totalLogicalSizeOfDataToCompactInBytesUndoneDueToPO:bigint,numIntermediateNodesClustered:bigint,numFilesSkippedAfterExpansion:bigint,totalSizeOfFilesSkippedAfterExpansionInBytes:bigint,totalLogicalSizeOfFilesSkippedAfterExpansionInBytes:bigint,totalSizeOfDataToRewriteInBytes:bigint,totalLogicalSizeOfDataToRewriteInBytes:bigint,timeMetrics:struct<classifierTimeMs:bigint,optimizerTimeMs:bigint,metadataLoadTimeMs:bigint,totalGetFilesForNodeCallsTimeMs:bigint,totalSamplingTimeMs:bigint,metadataCreationTimeMs:bigint>,maxOptimizeBatchesInParallel:bigint,currentIteration:int,maxIterations:int,clusteringStrategy:string>>]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"OPTIMIZE df_o_table ZORDER BY (user_id, event_time)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0a3de81-a1f4-4f45-8f48-2af1419ebc61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e87107b-1962-410b-add0-826e0f4e29c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### __4. Clean Up__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74a7589d-f0b5-4f9e-ae3a-5dc18f7e435c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[path: string]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview files older than 7 days (the default)\n",
    "spark.sql(\"VACUUM df_o_table RETAIN 168 HOURS DRY RUN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7306de22-a8b7-46ec-a416-fbc9d9f1e5a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[path: string]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Physically delete the old files\n",
    "spark.sql(\"VACUUM df_o_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d50dc071-2109-49a7-a91a-ce54a72c564d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49b7e531-86e6-497b-a178-6bf8cd8f1af9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Resources**\n",
    "- [OPTIMZE and VACCUM](https://docs.databricks.com/aws/en/delta/vacuum/)\n",
    "- [Tutorials](https://docs.databricks.com/delta/tutorial.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e6eb3fc-5a0b-4de3-a400-fcc80f5b2d9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "----"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5202199112327274,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Day_5",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}