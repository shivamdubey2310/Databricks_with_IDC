{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e62dcd08-2680-437a-b10c-4c9b6b2f58d9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 1"
    }
   },
   "source": [
    "# **PHASE 3: ADVANCED ANALYTICS (Days 9-11)**\n",
    "\n",
    "## **DAY 12 (20/01/26) - MLflow Basics**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7532b03a-927e-4e8a-9856-33d66f08e2ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Section 1 - Learn**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e8b751c-3188-4f16-b99b-334bf092a600",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **_1. MLflow components (tracking, registry, models)_**\n",
    "\n",
    "MLflow is an open-source platform designed to manage the end-to-end machine learning lifecycle. It is built around several core components that work together to move a model from a local script to a production-ready service.\n",
    "\n",
    "##### 1. MLflow Tracking\n",
    "\n",
    "This is the central logging component. It allows you to record and query experiments, which are essentially executions of your data science code (called **Runs**).\n",
    "\n",
    "* **What it tracks:** Parameters (hyperparameters), Metrics (accuracy, loss), Code version (Git hash), and Artifacts (plots, data files, or the model itself).\n",
    "* **Key Concept:** **Experiments** group multiple runs together, making it easy to compare different iterations and visualize performance trends through the MLflow UI.\n",
    "\n",
    "##### 2. MLflow Models\n",
    "\n",
    "This component provides a **standardized format** for packaging machine learning models so they can be used in various downstream tools—such as real-time serving via a REST API or batch inference on Apache Spark.\n",
    "\n",
    "* **Flavors:** Every MLflow Model is stored as a directory containing an `MLmodel` file. This file defines \"flavors\" (e.g., `python_function`, `sklearn`, `pytorch`) that tell different deployment tools how to understand the model.\n",
    "* **Signatures:** It allows you to define a \"Model Signature,\" which acts as a data contract specifying exactly what input types the model expects and what it will return.\n",
    "\n",
    "##### 3. MLflow Model Registry\n",
    "\n",
    "The Registry is a centralized model store, set of APIs, and UI to collaboratively manage the full lifecycle of an MLflow Model. It provides:\n",
    "\n",
    "* **Versioning:** Automatically tracks and manages different versions of a model (e.g., Version 1, Version 2).\n",
    "* **Stage Transitions:** Allows you to assign \"Stages\" to a model version, such as **Staging**, **Production**, or **Archived**.\n",
    "* **Lineage:** Links every registered model version back to the specific MLflow Run and experiment that created it, ensuring full reproducibility.\n",
    "\n",
    "\n",
    "##### Comparison Summary\n",
    "\n",
    "| Component | Primary Purpose | Key Output |\n",
    "| --- | --- | --- |\n",
    "| **Tracking** | Logging and comparing experiment results. | Metrics, Parameters, and Artifacts. |\n",
    "| **Models** | Packaging models for deployment across platforms. | Standardized `MLmodel` directory. |\n",
    "| **Registry** | Governing model versions and lifecycle stages. | Versioned, production-ready models. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06338551-afd0-41fe-b5d7-60242170a294",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d043ee4d-c347-477b-8152-f121b00d7b89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **_2. Experiment tracking_**\n",
    "\n",
    "In Databricks, **Experiment Tracking** is powered by a managed version of **MLflow**. It serves as a digital lab notebook that automatically records every detail of your machine learning training sessions—parameters, metrics, code versions, and resulting models—so you can compare them and reproduce results later.\n",
    "\n",
    "##### **1. Key Concepts: Experiments vs. Runs**\n",
    "\n",
    "* **Run:** A single execution of your model training code. During a run, you log \"ingredients\" (parameters) and \"results\" (metrics).\n",
    "* **Experiment:** A logical container that groups related runs. For example, you might have one experiment for \"Sales Forecasting\" and run it 50 times with different settings to find the best model.\n",
    "* **Artifacts:** Heavy files produced during a run, such as the trained model itself, feature importance plots, or CSVs of predictions.\n",
    "\n",
    "##### **2. Manual Logging vs. Autologging**\n",
    "\n",
    "There are two ways to track your work in Databricks:\n",
    "\n",
    "* **Autologging (Recommended):** Databricks can automatically detect when you use popular libraries (like Scikit-learn, TensorFlow, or PyTorch) and log your parameters and metrics without you writing extra code.\n",
    "    * **How to enable:** Use `mlflow.autolog()` at the start of your notebook.\n",
    "\n",
    "* **Manual Logging:** Use the MLflow API for full control over what gets saved.\n",
    "\n",
    "```python\n",
    "import mlflow\n",
    "\n",
    "# Start a tracked session\n",
    "with mlflow.start_run(run_name=\"RandomForest_v1\"):\n",
    "    # Log 'Ingredients'\n",
    "    mlflow.log_param(\"n_estimators\", 100)\n",
    "    mlflow.log_param(\"max_depth\", 5)\n",
    "    \n",
    "    # ... Train your model here ...\n",
    "    \n",
    "    # Log 'Results'\n",
    "    mlflow.log_metric(\"rmse\", 0.85)\n",
    "    mlflow.log_metric(\"r2_score\", 0.91)\n",
    "    \n",
    "    # Save the 'Model'\n",
    "    mlflow.sklearn.log_model(my_model, \"model_artifact\")\n",
    "\n",
    "```\n",
    "\n",
    "##### **3. The Experiment Sidebar**\n",
    "\n",
    "You don't need to leave your notebook to see your progress. The **Experiments Sidebar** (located on the right-hand side of the Databricks notebook UI) provides a live feed of all runs.\n",
    "\n",
    "* **Comparison View:** You can select multiple runs and click **\"Compare\"** to see a side-by-side table or a parallel coordinates plot that shows which hyperparameters led to the best accuracy.\n",
    "* **Reproduce with One Click:** Every run is linked to the exact version of the notebook that created it. If you find a great model from six months ago, you can jump back to that specific code snapshot instantly.\n",
    "\n",
    "##### **4. Best Practices for Tracking**\n",
    "\n",
    "* **Log Training Data Versions:** Use `mlflow.data` to log a reference to the specific Delta table version used for training. This ensures your model is 100% reproducible.\n",
    "* **Use Tags:** Add tags like `team: marketing` or `status: candidate` to your runs so you can filter through thousands of experiments later in the **MLflow UI**.\n",
    "* **Centrally Manage with Unity Catalog:** In modern Databricks (2025/2026), your experiments and models should be registered in **Unity Catalog** for centralized governance and access control.\n",
    "\n",
    "##### **Summary of Benefits**\n",
    "\n",
    "| Benefit | How it helps you |\n",
    "| --- | --- |\n",
    "| **Reproducibility** | Exactly recreate any past result by seeing the exact code, data, and environment used. |\n",
    "| **Model Selection** | Use visual charts to find the \"sweet spot\" for hyperparameters (e.g., learning rate). |\n",
    "| **Auditability** | Maintain a complete history of every model ever trained for compliance and debugging. |\n",
    "| **Collaboration** | Share an experiment link with a teammate so they can see your results without you sending screenshots. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7aab340-5657-4263-838b-2738d2005519",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc4ae1f6-7b52-4f7c-8abc-617065010a5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **_3. Model logging_**\n",
    "\n",
    "In Databricks, **Model Logging** is the process of saving a trained machine learning model along with its metadata (dependencies, schema, and environment) into the MLflow tracking system.\n",
    "\n",
    "Unlike simply saving a file (like a `.pkl` or `.h5`), logging a model creates a standardized **MLflow Model**—a directory that contains everything needed for someone else (or a production server) to run the model without manually installing libraries.\n",
    "\n",
    "##### **1. The Logging Workflow**\n",
    "\n",
    "There are two primary ways to log your models:\n",
    "\n",
    "* **Autologging (Fastest):** Most popular libraries (Scikit-learn, XGBoost, PyTorch, etc.) are supported. By calling `mlflow.autolog()`, Databricks automatically captures parameters, metrics, and the model itself when you call `.fit()`.\n",
    "* **Manual Logging (Best for Production):** Provides granular control. You explicitly define what to save using `mlflow.<flavor>.log_model()`.\n",
    "\n",
    "```python\n",
    "import mlflow\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Standard manual logging pattern\n",
    "with mlflow.start_run():\n",
    "    model = RandomForestClassifier(n_estimators=100)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Log the model specifically for scikit-learn\n",
    "    mlflow.sklearn.log_model(\n",
    "        sk_model=model, \n",
    "        artifact_path=\"model_folder\",\n",
    "        input_example=X_train.iloc[[0]],  # Highly recommended for Unity Catalog\n",
    "        registered_model_name=\"sales_predictor\" # Optional: auto-registers the model\n",
    "    )\n",
    "\n",
    "```\n",
    "\n",
    "##### **2. Logged Model vs. Artifact**\n",
    "\n",
    "It is important to distinguish between `log_artifact()` and `log_model()`:\n",
    "\n",
    "* **`log_artifact()`**: Saves a specific file (e.g., a `.png` plot of feature importance or a `.csv` of predictions). It is just a \"dumb\" file storage.\n",
    "* **`log_model()`**: Saves a \"smart\" package. It includes an `MLmodel` configuration file, a `conda.yaml` for environment reproduction, and the model binary. This allows for **Model Serving** (turning the model into a REST API) with one click.\n",
    "\n",
    "##### **3. Why Signatures Matter**\n",
    "\n",
    "In modern Databricks (2025/2026), **Model Signatures** are critical. A signature defines the expected input and output data types (e.g., \"Input: 3 Floats, Output: 1 Boolean\").\n",
    "\n",
    "* **Unity Catalog Requirement:** You cannot register a model in Unity Catalog without a signature.\n",
    "* **The \"Easy\" Way:** Pass an `input_example` when logging (as shown in the code above). MLflow will automatically infer the signature from that example.\n",
    "\n",
    "##### **4. Best Practices**\n",
    "\n",
    "* **Log Dependencies:** Always check the `requirements.txt` generated by MLflow to ensure custom libraries (like a specific version of `pandas`) are included.\n",
    "* **Use Unity Catalog:** Instead of the legacy Workspace Model Registry, log and register your models directly into **Unity Catalog** for better security and cross-workspace access.\n",
    "* **Add Descriptions:** Use the `description` parameter in `log_model()` to explain what the version does (e.g., \"Updated with Q4 seasonal data\").\n",
    "\n",
    "##### **Summary Comparison**\n",
    "\n",
    "| Feature | `log_artifact()` | `log_model()` |\n",
    "| --- | --- | --- |\n",
    "| **Object Type** | Generic files (images, logs, csvs) | Standardized MLflow Model |\n",
    "| **Reproducibility** | Manual effort required | Automated (packages dependencies) |\n",
    "| **Serving** | Not supported | Integrated with Databricks Serving |\n",
    "| **Unity Catalog** | Not registerable | Required for Model Registry |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64198984-5d5b-4a51-936b-a237fd10e5be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e6816f6-7d6f-4983-81a3-107551655a99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **_4. MLflow UI_**\n",
    "\n",
    "The **MLflow Tracking UI** is the visual command center for your machine learning lifecycle. In Databricks, it provides a centralized interface to visualize, search, and compare the \"DNA\" of your models—every parameter, metric, and code version you've ever logged.\n",
    "\n",
    "##### **1. Key Components of the UI**\n",
    "\n",
    "The UI is organized into a hierarchy that mirrors how you work:\n",
    "\n",
    "* **Experiments Page:** A high-level list of all your projects. Each experiment acts as a folder containing related training attempts.\n",
    "* **Runs Table:** A spreadsheet-like view of every individual training session. You can **filter**, **sort**, and **search** for runs by specific metrics (e.g., `accuracy > 0.9`).\n",
    "* **Run Details Page:** Clicking on a run name opens a deep dive into that specific session. It includes:\n",
    "* **Parameters:** What \"ingredients\" were used (e.g., `learning_rate: 0.01`).\n",
    "* **Metrics:** How the \"recipe\" turned out (e.g., `F1-Score: 0.88`).\n",
    "* **Artifacts:** The resulting files, including the **Model binary**, **Conda environment**, and **Feature Importance plots**.\n",
    "* **Traceability:** A direct link to the exact **Notebook version** and **Git commit** that produced the results.\n",
    "\n",
    "\n",
    "##### **2. Advanced Visual Comparison**\n",
    "\n",
    "One of the UI's most powerful features is the **Chart View**. Instead of looking at raw numbers, you can select multiple runs and generate:\n",
    "\n",
    "* **Parallel Coordinates Plots:** Visualize how different combinations of hyperparameters (like `depth` and `estimators`) impact your final performance.\n",
    "* **Scatter Plots:** Compare two metrics (e.g., `Latency` vs. `Accuracy`) to find the most efficient model.\n",
    "* **Bar Charts:** Side-by-side comparisons of key KPIs for candidate models.\n",
    "\n",
    "##### **3. MLflow UI vs. Notebook Sidebar**\n",
    "\n",
    "Databricks offers two ways to view your MLflow data:\n",
    "\n",
    "| Feature | Notebook Experiment Sidebar | Full MLflow UI |\n",
    "| --- | --- | --- |\n",
    "| **Location** | Right-hand side of your notebook. | Left-hand sidebar under **Mosaic AI > Experiments**. |\n",
    "| **Best For...** | Quick checks while coding. | Deep analysis and multi-run comparison. |\n",
    "| **Functionality** | Shows recent runs for *that* notebook only. | Shows *all* runs across all notebooks/jobs. |\n",
    "| **Visuals** | Basic list of parameters/metrics. | Advanced charts, search, and artifact browsing. |\n",
    "\n",
    "\n",
    "##### **4. Accessing the UI in Databricks**\n",
    "\n",
    "1. **Direct Sidebar:** In the left-hand navigation menu, click **Experiments** (the laboratory beaker icon).\n",
    "2. **From a Notebook:** Click the **Experiment** icon in the top right of your notebook to open the sidebar, then click the \"External Link\" icon to jump to the full UI.\n",
    "3. **From Unity Catalog:** If you've registered a model, the **Catalog Explorer** provides links back to the original MLflow run and experiment that created it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8addbe16-6bbb-4ca7-b2cc-0668ba62bca3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20f23c91-cbe8-4f2e-9e7e-9f255237199a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Practice**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfb66933-8549-4f61-9ac8-8d34e5f25bc3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c21086e-7ca8-4253-bb99-63e35b3ee023",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mThe Python process exited with exit code 137 (SIGKILL: Killed). This may have been caused by an OOM error. Check your command's memory usage.\u001B[0m\n",
       "\u001B[0;31m\u001B[0m\n",
       "\u001B[0;31m\u001B[0m\n",
       "\u001B[0;31m\u001B[0m\n",
       "\u001B[0;31mThe last 10 KB of the process's stderr and stdout can be found below. See driver logs for full logs.\u001B[0m\n",
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mLast messages on stderr:\u001B[0m\n",
       "\u001B[0;31mWed Jan 21 10:42:26 2026 Connection to spark from PID  2562\u001B[0m\n",
       "\u001B[0;31mWed Jan 21 10:42:27 2026 Initialized gateway on port 36113\u001B[0m\n",
       "\u001B[0;31mWed Jan 21 10:42:27 2026 Connected to spark.\u001B[0m\n",
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mLast messages on stdout:\u001B[0m\n",
       "\u001B[0;31mNOTE: When using the `ipython kernel` entry point, Ctrl-C will not work.\u001B[0m\n",
       "\u001B[0;31m\u001B[0m\n",
       "\u001B[0;31mTo exit, you will have to explicitly quit this process, by either sending\u001B[0m\n",
       "\u001B[0;31m\"quit\" from a client, or using Ctrl-\\ in UNIX-like environments.\u001B[0m\n",
       "\u001B[0;31m\u001B[0m\n",
       "\u001B[0;31mTo read more about this, see https://github.com/ipython/ipython/issues/2049\u001B[0m\n",
       "\u001B[0;31m\u001B[0m\n",
       "\u001B[0;31m\u001B[0m\n",
       "\u001B[0;31mTo connect another client to this kernel, use:\u001B[0m\n",
       "\u001B[0;31m    --existing /databricks/kernel-connections/6c50a913c3db0ab7d0ed5f77e0187dc1b679c5e40f839b1aaaa28a13a293bfe0.json\u001B[0m\n",
       "\u001B[0;31m1\u001B[0m"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": ""
       },
       "removedWidgets": [],
       "sqlProps": {
        "breakingChangeInfo": null,
        "errorClass": "PYTHON_KERNEL_UNRESPONSIVE",
        "pysparkCallSite": null,
        "pysparkFragment": null,
        "pysparkSummary": null,
        "sqlState": "XXD02",
        "stackTrace": null,
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mThe Python process exited with exit code 137 (SIGKILL: Killed). This may have been caused by an OOM error. Check your command's memory usage.\u001B[0m\n\u001B[0;31m\u001B[0m\n\u001B[0;31m\u001B[0m\n\u001B[0;31m\u001B[0m\n\u001B[0;31mThe last 10 KB of the process's stderr and stdout can be found below. See driver logs for full logs.\u001B[0m\n\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mLast messages on stderr:\u001B[0m\n\u001B[0;31mWed Jan 21 10:42:26 2026 Connection to spark from PID  2562\u001B[0m\n\u001B[0;31mWed Jan 21 10:42:27 2026 Initialized gateway on port 36113\u001B[0m\n\u001B[0;31mWed Jan 21 10:42:27 2026 Connected to spark.\u001B[0m\n\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mLast messages on stdout:\u001B[0m\n\u001B[0;31mNOTE: When using the `ipython kernel` entry point, Ctrl-C will not work.\u001B[0m\n\u001B[0;31m\u001B[0m\n\u001B[0;31mTo exit, you will have to explicitly quit this process, by either sending\u001B[0m\n\u001B[0;31m\"quit\" from a client, or using Ctrl-\\ in UNIX-like environments.\u001B[0m\n\u001B[0;31m\u001B[0m\n\u001B[0;31mTo read more about this, see https://github.com/ipython/ipython/issues/2049\u001B[0m\n\u001B[0;31m\u001B[0m\n\u001B[0;31m\u001B[0m\n\u001B[0;31mTo connect another client to this kernel, use:\u001B[0m\n\u001B[0;31m    --existing /databricks/kernel-connections/6c50a913c3db0ab7d0ed5f77e0187dc1b679c5e40f839b1aaaa28a13a293bfe0.json\u001B[0m\n\u001B[0;31m1\u001B[0m"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Prepare data\n",
    "df = spark.table(\"workspace.gold.products\").toPandas()\n",
    "X = df[[\"views\", \"cart_adds\"]]\n",
    "y = df[\"purchases\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f44d3e74-876d-4ca3-9738-e97fa43041af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# MLflow experiment\n",
    "with mlflow.start_run(run_name=\"linear_regression_v1\"):\n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"model_type\", \"LinearRegression\")\n",
    "    mlflow.log_param(\"test_size\", 0.2)\n",
    "\n",
    "    # Train\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate\n",
    "    score = model.score(X_test, y_test)\n",
    "    mlflow.log_metric(\"r2_score\", score)\n",
    "\n",
    "    # Log model\n",
    "    mlflow.sklearn.log_model(model, \"model\")\n",
    "\n",
    "print(f\"R² Score: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d48c73b-db9c-4b2c-9272-df9008b573ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c599c04-2228-423b-a454-c6f2df98a91f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **_Resources_**\n",
    "\n",
    "- [Mlflow](https://docs.databricks.com/mlflow/)\n",
    "- [MLflow with databricks](https://youtu.be/ds__AEIqUfE?si=b3EB01LmOB5suO2v)\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5202199112327274,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Day_12",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}