{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e62dcd08-2680-437a-b10c-4c9b6b2f58d9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 1"
    }
   },
   "source": [
    "# **PHASE 2: DATA ENGINEERING (Days 5-8)**\n",
    "\n",
    "## **DAY 7 (15/01/26) - Workflows & Job Orchestration**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7532b03a-927e-4e8a-9856-33d66f08e2ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Section 1 - Learn**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e8b751c-3188-4f16-b99b-334bf092a600",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **_1. Databricks Jobs vs notebooks_**\n",
    "\n",
    "In Databricks, **Notebooks** are for development and exploration, while **Jobs** are for production and automation. Think of the Notebook as the \"draft\" and the Job as the \"scheduled execution.\"\n",
    "\n",
    "##### **1. Databricks Notebooks (Development)**\n",
    "\n",
    "* **Interactive Coding:** Designed for a \"human-in-the-loop\" experience where you run cells one by one and see results (tables/charts) immediately.\n",
    "* **Collaboration:** Multiple users can edit the same notebook simultaneously (like Google Docs) and leave comments.\n",
    "* **Manual Execution:** Typically run on **All-Purpose Clusters**, which are always \"on\" and more expensive but provide instant response for testing.\n",
    "* **Flexibility:** Easily switch between Python, SQL, Scala, and R within the same file using magic commands (`%sql`).\n",
    "* **Visualizations:** Includes built-in `display()` functions to render interactive plots and data profiles for quick analysis.\n",
    "\n",
    "##### **2. Databricks Jobs (Production)**\n",
    "\n",
    "* **Automation:** Used to schedule notebooks, JARs, or Python scripts to run at specific times (CRON) or in a sequence (Workflows).\n",
    "* **Job Clusters (Transient):** Jobs usually run on **Job Clusters**, which spin up when the task starts and shut down immediately after. These are **significantly cheaper** (approx. half the cost) than All-Purpose clusters.\n",
    "* **Orchestration:** Jobs allow you to create **DAGs (Directed Acyclic Graphs)** of tasksâ€”for example: \"Run Ingest Notebook -> if success, run Transform Notebook -> if failure, send Email.\"\n",
    "* **Reliability:** Includes built-in retry logic, timeout settings, and sophisticated alerting (Email/Slack/PagerDuty) if a task fails.\n",
    "* **Parameters:** You can pass different values (like `date` or `region`) into a Job at runtime using **Widgets**, allowing one notebook to serve many different scheduled tasks.\n",
    "\n",
    "##### **Key Comparison Table**\n",
    "\n",
    "| Feature | Notebooks | Jobs (Workflows) |\n",
    "| --- | --- | --- |\n",
    "| **Primary Goal** | Exploration & Prototyping | Production & Automation |\n",
    "| **Compute Type** | All-Purpose Cluster (Interactive) | Job Cluster (Transient/Short-lived) |\n",
    "| **Cost** | High ($$ per DBU) | Low ($ per DBU) |\n",
    "| **Execution** | Manual / Cell-by-cell | Scheduled / Automated Trigger |\n",
    "| **Error Handling** | Manual debugging | Auto-retries & Notifications |\n",
    "| **Version Control** | Integrated Repos (Git) | Runs specific Git commits/tags |\n",
    "\n",
    "##### **The \"Best Practice\" Workflow**\n",
    "\n",
    "1. **Develop** your logic interactively in a **Notebook** using a small sample of data.\n",
    "2. **Modularize** the code so it can accept parameters (e.g., using `dbutils.widgets`).\n",
    "3. **Deploy** that notebook by creating a **Job** that runs it on a schedule against the full production dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06338551-afd0-41fe-b5d7-60242170a294",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d043ee4d-c347-477b-8152-f121b00d7b89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **_2. Multi-task workflows_**\n",
    "\n",
    "In Databricks, a **Multi-task Workflow** allows you to orchestrate complex data pipelines by stringing together multiple individual tasks into a **Directed Acyclic Graph (DAG)**. This replaces the need for external tools like Airflow for many Lakehouse use cases.\n",
    "\n",
    "##### **1. Core Concepts of Multi-task Jobs**\n",
    "\n",
    "* **Task Dependencies:** You can define the order of execution. For example, the \"Silver\" task only starts if the \"Bronze\" task completes successfully.\n",
    "* **Task Types:** A single workflow can mix different types of tasks, including **Notebooks**, **Python scripts**, **SQL queries**, **Delta Live Tables**, and even **dbt** projects.\n",
    "* **Shared Compute:** Multiple tasks can run on the same **Job Cluster** to save on startup time, or you can assign different clusters to different tasks based on their resource needs (e.g., a memory-heavy cluster for ML training and a small one for SQL).\n",
    "\n",
    "##### **2. Key Features for Production**\n",
    "\n",
    "* **Conditional Execution:** Use \"If/Else\" logic within the workflow (e.g., \"If the data quality check fails, run the Quarantine task; otherwise, run the Gold task\").\n",
    "* **Task Values:** Tasks can \"talk\" to each other. You can set a value in Task A (like a file path or a record count) and retrieve it in Task B using `dbutils.jobs.taskValues`.\n",
    "* **Repair and Rerun:** If a workflow with 10 tasks fails at task #8, you don't have to restart the whole thing. You can **\"Repair\"** the run, and Databricks will only execute the failed task and those that follow it.\n",
    "* **Matrix Tasks:** Allows you to run the same task multiple times in parallel with different parameters (e.g., running the same \"Report\" notebook for 50 different countries simultaneously).\n",
    "\n",
    "##### **3. Workflow Triggers**\n",
    "\n",
    "* **Scheduled:** Traditional CRON-based timing (e.g., every day at 2:00 AM).\n",
    "* **File Arrival:** Triggered automatically as soon as new files land in your cloud storage (S3/ADLS).\n",
    "* **Continuous:** The job restarts immediately after it finishes, ideal for near real-time processing.\n",
    "* **API-based:** Triggered via an external call (e.g., from an Azure Data Factory pipeline or a GitHub Action).\n",
    "\n",
    "##### **4. Best Practices for Design**\n",
    "\n",
    "* **Modularity:** Keep your notebooks small and focused on one goal (e.g., one notebook for Bronze, one for Silver). This makes debugging much easier.\n",
    "* **Use Job Clusters:** Always use **New Job Clusters** for production to take advantage of the lower DBU pricing compared to All-Purpose clusters.\n",
    "* **Notifications:** Configure **System Notifications** for specific events (e.g., On Failure, On Duration Exceeded) to ensure your team is alerted before the business notices a delay.\n",
    "* **Timeouts:** Set a maximum completion time for each task to prevent a \"stuck\" process from wasting money and blocking the rest of the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7aab340-5657-4263-838b-2738d2005519",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc4ae1f6-7b52-4f7c-8abc-617065010a5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **_3. Parameters & scheduling_**\n",
    "\n",
    "In Databricks, **Parameters** allow you to make your code dynamic, while **Scheduling** ensures your logic runs at the right time without manual intervention.\n",
    "\n",
    "##### **1. Parameters using Widgets**\n",
    "\n",
    "Widgets are the primary way to pass variables into a Databricks notebook. They allow the same notebook to be reused for different dates, regions, or data sources.\n",
    "\n",
    "* **Types of Widgets:** You can create text boxes, dropdown menus, or multi-select lists at the top of your notebook.\n",
    "* **Default Values:** Always set a default value in your code so the notebook can run even if no parameter is provided.\n",
    "* **Accessing Values:** Use the `dbutils.widgets` API to retrieve the value passed by the user or the Job.\n",
    "* **SQL Integration:** You can access these parameters directly in SQL cells using the `$parameter_name` syntax.\n",
    "\n",
    "**Example (Python):**\n",
    "\n",
    "```python\n",
    "# Create a text widget\n",
    "dbutils.widgets.text(\"run_date\", \"2026-01-01\")\n",
    "\n",
    "# Get the value into a variable\n",
    "current_date = dbutils.widgets.get(\"run_date\")\n",
    "\n",
    "# Use it in a query\n",
    "df = spark.table(\"sales\").filter(F.col(\"date\") == current_date)\n",
    "\n",
    "```\n",
    "\n",
    "##### **2. Scheduling Patterns**\n",
    "\n",
    "Scheduling turns your interactive notebook into a production pipeline.\n",
    "\n",
    "* **CRON Expressions:** Use standard CRON syntax for complex timing (e.g., \"0 30 2 * * ?\" for every day at 2:30 AM).\n",
    "* **Simple Scheduler:** A user-friendly UI for selecting frequency (Minutes, Hours, Days, Weeks).\n",
    "* **Time Zone Awareness:** Ensure you set the schedule to the correct time zone (usually UTC for global teams) to avoid confusion during Daylight Savings.\n",
    "* **Concurrent Runs:** You can control whether a second run should start if the first one is still running (**Concurrency Limit**).\n",
    "\n",
    "##### **3. Advanced Triggering**\n",
    "\n",
    "Beyond simple time-based schedules, Databricks supports event-driven triggers:\n",
    "\n",
    "* **File Arrival Trigger:** The job starts as soon as a new file is detected in a specific S3 bucket or Azure Container. This is much more efficient than \"polling\" every hour.\n",
    "* **Continuous Execution:** The job runs in a loop. As soon as one run finishes, the next one starts. This is best for low-latency Delta Live Tables.\n",
    "* **API Triggering:** Use the **Jobs API** to trigger runs from external tools like Airflow, Azure Data Factory, or GitHub Actions.\n",
    "\n",
    "##### **4. Best Practices**\n",
    "\n",
    "* **Parameterized Paths:** Don't hardcode file paths. Use a parameter for the \"environment\" (e.g., `/mnt/data/${env}/orders`) so the same job can run in Dev, Test, and Prod.\n",
    "* **Job Parameters vs. Task Parameters:** You can set parameters at the **Job level** (available to all tasks) or the **Task level** (specific to one notebook).\n",
    "* **Monitoring Alerts:** Always set an \"On Failure\" alert to notify your team via Email or Slack if a scheduled run fails.\n",
    "* **Timeout Settings:** Define a timeout for scheduled jobs (e.g., 2 hours) so that a \"stuck\" job doesn't run forever and consume your entire budget."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8addbe16-6bbb-4ca7-b2cc-0668ba62bca3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ffdd0ba-f92d-4e14-941c-620f2fb6cc1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **_4. Error handling_**\n",
    "\n",
    "Error handling in Databricks is critical for transitioning from \"it works on my machine\" to a robust production pipeline. It involves a mix of standard Python try-except blocks and Databricks-specific orchestration features.\n",
    "\n",
    "##### **1. Notebook-Level Error Handling (Python)**\n",
    "\n",
    "* **Try-Except Blocks:** Use these to catch specific Spark exceptions (like `AnalysisException` for missing tables) and log them without crashing the entire cluster.\n",
    "* **Graceful Failures:** Instead of letting a job crash, use `dbutils.notebook.exit(\"Error Message\")` to stop a notebook gracefully and pass a status message back to the parent Job.\n",
    "* **Assertion Checks:** Use `assert` or `if/raise` to validate data before processing (e.g., \"Raise error if the input DataFrame is empty\").\n",
    "\n",
    "##### **2. Workflow/Job-Level Handling**\n",
    "\n",
    "* **Retries:** Configure the **Max Retries** setting in the Job UI. If a task fails due to a \"spot instance\" being reclaimed or a temporary network glitch, Databricks will automatically try again.\n",
    "* **Timeout Limits:** Set a **Timeout** for every task. If a notebook gets stuck in an infinite loop or a \"deadlock,\" Databricks will kill the task rather than letting it burn your budget.\n",
    "* **On-Failure Tasks:** Create a specific task in your Workflow that only runs if a previous task fails. This is commonly used to send a custom Slack/Email alert or \"clean up\" temporary files.\n",
    "\n",
    "##### **3. Delta Lake \"ACID\" Safety**\n",
    "\n",
    "* **Automatic Rollbacks:** Because Delta is ACID compliant, if an error occurs mid-write, Delta Lake automatically ensures no partial data is saved. You don't need to write manual \"cleanup\" code for failed writes.\n",
    "* **Expectations (DLT):** In Delta Live Tables, you can define **Expectations** to handle bad data:\n",
    "* `expect`: Track the error but allow the record.\n",
    "* `expect or drop`: Remove the specific record that failed.\n",
    "* `expect or fail`: Crash the entire pipeline if a critical error occurs.\n",
    "\n",
    "\n",
    "\n",
    "##### **4. Logging and Monitoring**\n",
    "\n",
    "* **Log4j / Python Logging:** Use the standard `logging` library to send custom logs to the **Driver Logs** tab in the cluster UI.\n",
    "* **Query Watchdog:** Enable the \"Query Watchdog\" in cluster settings to automatically kill queries that are taking up too many resources or producing an accidental \"Cartesian Product\" (huge join).\n",
    "\n",
    "##### **Common Pattern: The \"Try-Except-Exit\" Template**\n",
    "\n",
    "```python\n",
    "try:\n",
    "    # Your Spark logic here\n",
    "    df = spark.read.table(\"bronze_data\")\n",
    "    df.write.mode(\"append\").saveAsTable(\"silver_data\")\n",
    "    \n",
    "except Exception as e:\n",
    "    # Log the error and exit with a failure message for the Job UI\n",
    "    error_msg = f\"Task failed: {str(e)}\"\n",
    "    print(error_msg)\n",
    "    dbutils.notebook.exit(error_msg) \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "340c3f4a-2937-44b6-80dc-01ca6e6af4b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20f23c91-cbe8-4f2e-9e7e-9f255237199a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Practice**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9f15bb0-92a0-485b-bd7b-7bb97e6afcc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfb66933-8549-4f61-9ac8-8d34e5f25bc3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def load_ecommerce_dataset(Month_name):\n",
    "    df = spark.read.csv(f\"/Volumes/workspace/ecommerce/ecommerce_data/2019-{Month_name}.csv\", header=True, inferSchema=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c21086e-7ca8-4253-bb99-63e35b3ee023",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df_n = load_ecommerce_dataset(\"Nov\")\n",
    "df_o = load_ecommerce_dataset(\"Oct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d8e9a32-301b-44f8-9c25-e97c74c285cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### **1. Add parameter widgets to notebooks** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45312d20-6835-4f23-a753-f51dae4722ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Add widgets for parameters\n",
    "dbutils.widgets.text(\"source_path\", \"workspace.silver.ecommerce_cleaned\")\n",
    "dbutils.widgets.dropdown(\"layer\", \"bronze\", [\"bronze\",\"silver\",\"gold\"])\n",
    "dbutils.widgets.dropdown(\"month\", \"Oct\", [\"Oct\", \"Nov\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e15da70-2cd4-4db6-afeb-aa618ed6aaf5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Use parameters\n",
    "source = dbutils.widgets.get(\"source_path\")\n",
    "layer = dbutils.widgets.get(\"layer\")\n",
    "month = dbutils.widgets.get(\"month\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a473b520-ae59-479e-b068-d5b85545f949",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workspace.silver.ecommerce_cleaned\nsilver\nOct\n"
     ]
    }
   ],
   "source": [
    "print(source)\n",
    "print(layer)\n",
    "print(month)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d50dc071-2109-49a7-a91a-ce54a72c564d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49b7e531-86e6-497b-a178-6bf8cd8f1af9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Resources**\n",
    "- [Databricks Jobs](https://docs.databricks.com/jobs/)\n",
    "- [Parameters](https://docs.databricks.com/jobs/parameters.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e6eb3fc-5a0b-4de3-a400-fcc80f5b2d9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "----"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5202199112327274,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Day_7_0",
   "widgets": {
    "layer": {
     "currentValue": "silver",
     "nuid": "bb2fe094-6aea-42ec-b8d1-d8a95045d0de",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "bronze",
      "label": null,
      "name": "layer",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "bronze",
        "silver",
        "gold"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "bronze",
      "label": null,
      "name": "layer",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "bronze",
        "silver",
        "gold"
       ]
      }
     }
    },
    "month": {
     "currentValue": "Oct",
     "nuid": "1eb336d2-cbbb-4729-86ed-e72967cad9a7",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "Oct",
      "label": null,
      "name": "month",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "Oct",
        "Nov"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "Oct",
      "label": null,
      "name": "month",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "Oct",
        "Nov"
       ]
      }
     }
    },
    "source_path": {
     "currentValue": "workspace.silver.ecommerce_cleaned",
     "nuid": "1a6451d3-2402-4dc8-83bf-bc89a93280de",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "workspace.silver.ecommerce_cleaned",
      "label": null,
      "name": "source_path",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "workspace.silver.ecommerce_cleaned",
      "label": null,
      "name": "source_path",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}