{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e62dcd08-2680-437a-b10c-4c9b6b2f58d9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 1"
    }
   },
   "source": [
    "# **PHASE 3: ADVANCED ANALYTICS (Days 9-11)**\n",
    "\n",
    "## **DAY 10 (18/01/26) - Performance Optimization**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7532b03a-927e-4e8a-9856-33d66f08e2ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Section 1 - Learn**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e8b751c-3188-4f16-b99b-334bf092a600",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **_1. SQL Warehouse_**\n",
    "\n",
    "Understanding **Query Execution Plans** is essential for debugging slow Spark or SQL code. In Databricks, the Execution Plan is the \"blueprint\" Spark creates to decide how to read, join, and aggregate your data across the cluster.\n",
    "\n",
    "##### **1. The Spark Optimizer (Catalyst)**\n",
    "\n",
    "When you run a query, Spark doesn't execute it immediately. It passes your code through the **Catalyst Optimizer**, which transforms it through four stages:\n",
    "\n",
    "1. **Analysis:** Checks the SQL/Code for syntax errors and ensures tables/columns exist in the Catalog.\n",
    "2. **Logical Optimization:** Applies rule-based optimizations, like \"Pushdown Filters\" (filtering data as early as possible so less data is moved).\n",
    "3. **Physical Planning:** Generates multiple physical strategies (e.g., deciding between a Broadcast Join vs. a Shuffle Hash Join) and picks the one with the lowest \"cost.\"\n",
    "4. **Code Generation (Tungsten):** Generates highly optimized Java bytecode to run on the workers.\n",
    "\n",
    "##### **2. How to View the Plan**\n",
    "\n",
    "* **SQL:** Use the command `EXPLAIN EXTENDED SELECT ...`\n",
    "* **PySpark:** Use `df.explain(True)`\n",
    "* **Spark UI:** The \"SQL\" tab in the Spark UI provides a visual DAG (Directed Acyclic Graph) of the execution, which is the most user-friendly way to spot bottlenecks.\n",
    "\n",
    "##### **3. Key Terms to Look For**\n",
    "\n",
    "When reading a plan, you want to identify these specific operations:\n",
    "\n",
    "| Operation | What it means | Performance Impact |\n",
    "| --- | --- | --- |\n",
    "| **FileScan** | Reading data from storage (S3/ADLS). | Check \"PartitionFilters\" and \"PushedFilters\" to see if data skipping is working. |\n",
    "| **Exchange** | A **Shuffle**. Data is being moved across the network between nodes. | **Expensive.** High exchange usually means a large join or a wide group-by. |\n",
    "| **BroadcastExchange** | Sending a small table to all nodes. | **Fast.** Much better than a full shuffle for joins. |\n",
    "| **Project** | Selecting specific columns. | Low cost. |\n",
    "| **Filter** | Filtering rows. | Low cost; highly efficient if pushed down to the data source. |\n",
    "\n",
    "##### **4. Red Flags in a Query Plan**\n",
    "\n",
    "* **CartesianProduct:** This happens if you join two tables without a join key. It causes an exponential explosion of data and usually crashes the cluster.\n",
    "* **ObjectHashAggregate:** Usually faster than `SortAggregate`, but if you see `Sort`, it means Spark had to sort the data on disk first, which is slower.\n",
    "* **Scan with no Filters:** If you are querying a massive table but the plan shows no `PartitionFilters`, Spark is doing a \"Full Table Scan,\" which is extremely inefficient.\n",
    "\n",
    "##### **5. Performance Tuning via the Plan**\n",
    "\n",
    "If you see a massive **Exchange** node (Shuffle) that is slowing you down:\n",
    "\n",
    "1. **Check Join Types:** Can you use a `broadcast()` hint for the smaller table?\n",
    "2. **Z-Order:** If the `FileScan` is taking too long, does the table need to be Z-Ordered on the filter column?\n",
    "3. **Data Skew:** If one task in the Spark UI takes 10 minutes while others take 10 seconds, you have \"Data Skew.\" You may need to use a `SKEW` hint or salt your keys."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06338551-afd0-41fe-b5d7-60242170a294",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d043ee4d-c347-477b-8152-f121b00d7b89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **_2. Partitioning strategies_**\n",
    "\n",
    "In Databricks, partitioning is the process of physically grouping data into folders based on a specific column (like `/year=2026/month=01/`). While it was once the standard for all big data tables, modern best practices have shifted toward more automated solutions.\n",
    "\n",
    "##### **1. The Golden Rule: 1 TB**\n",
    "\n",
    "* **Small Tables (< 1 TB):** Do **not** partition. The overhead of managing separate folders and files actually slows down queries. Spark's metadata handling is efficient enough to scan 1 TB without partitioning.\n",
    "* **Large Tables (> 1 TB):** Partitioning becomes beneficial because it allows for **Partition Pruning**, where Spark skips entire directories of data that don't match your query.\n",
    "\n",
    "##### **2. Choosing the Right Column**\n",
    "\n",
    "Choosing the wrong column can lead to the **\"Small File Problem\"** or unbalanced data.\n",
    "\n",
    "* **Low Cardinality:** Choose columns with a limited number of unique values (e.g., `Date`, `Region`, `Department`).\n",
    "* **Avoid High Cardinality:** Never partition by `User_ID`, `Transaction_ID`, or `Timestamp`. If a column has more than 10,000 unique values, partitioning will create too many tiny folders, crushing performance.\n",
    "* **Uniformity:** Ensure your partitions are roughly the same size. If 90% of your data is in one \"Region,\" partitioning by region won't help much.\n",
    "\n",
    "##### **3. Liquid Clustering: The Modern Alternative**\n",
    "\n",
    "In 2026, Databricks recommends **Liquid Clustering** over traditional partitioning for almost all new tables.\n",
    "\n",
    "| Feature | Traditional Partitioning | Liquid Clustering |\n",
    "| --- | --- | --- |\n",
    "| **Setup** | Fixed at table creation. | Flexible; can be changed anytime. |\n",
    "| **Maintenance** | Manual (needs `ZORDER` frequently). | Automatic (predictive optimization). |\n",
    "| **Cardinality** | Only for low cardinality. | Works for high and low cardinality. |\n",
    "| **Data Skew** | Vulnerable to uneven partitions. | Naturally handles data skew. |\n",
    "\n",
    "**Best Practice:** Use Liquid Clustering (`CLUSTER BY`) unless you have a legacy requirement for specific folder structures or a very static, massive table (100 TB+) with a single, predictable filter.\n",
    "\n",
    "##### **4. Implementation Example**\n",
    "\n",
    "**Traditional Partitioning (Legacy/Static):**\n",
    "\n",
    "```python\n",
    "df.write \\\n",
    "  .format(\"delta\") \\\n",
    "  .partitionBy(\"event_date\") \\\n",
    "  .saveAsTable(\"events_partitioned\")\n",
    "\n",
    "```\n",
    "\n",
    "**Liquid Clustering (Recommended):**\n",
    "\n",
    "```sql\n",
    "-- Modern SQL approach\n",
    "CREATE TABLE events_clustered\n",
    "USING DELTA\n",
    "CLUSTER BY (customer_id, event_date);\n",
    "\n",
    "```\n",
    "\n",
    "##### **Summary Table**\n",
    "\n",
    "| Strategy | When to Use |\n",
    "| --- | --- |\n",
    "| **No Partitioning** | Tables under 1 TB. |\n",
    "| **Partitioning** | Large tables (> 1 TB) with fixed, low-cardinality filters. |\n",
    "| **Liquid Clustering** | **Default for 2026.** Evolving schemas, high cardinality, or skewed data. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7aab340-5657-4263-838b-2738d2005519",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc4ae1f6-7b52-4f7c-8abc-617065010a5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **_3. OPTIMIZE & ZORDER_**\n",
    "\n",
    "In Delta Lake, `OPTIMIZE` and `ZORDER` are the two primary tools used to physically reorganize data on disk to improve query performance and reduce storage overhead.\n",
    "\n",
    "##### **1. OPTIMIZE (File Compaction)**\n",
    "\n",
    "* **The \"Small File Problem\":** Frequent streaming or small batch writes often create thousands of tiny Parquet files. This slows down queries because the engine spends more time opening/closing files and reading metadata than actually processing data.\n",
    "* **Bin-Packing:** The `OPTIMIZE` command performs \"compaction.\" It takes those small files and merges them into larger, right-sized files (defaulting to **1GB**).\n",
    "* **Metadata Efficiency:** By reducing the number of files, Spark has much less work to do when scanning the transaction log and listing files in cloud storage.\n",
    "\n",
    "##### **2. ZORDER (Data Clustering)**\n",
    "\n",
    "* **Multi-Dimensional Sorting:** While `OPTIMIZE` just makes files bigger, adding `ZORDER BY` physically **rearranges the rows** within those files based on specific columns.\n",
    "* **Data Skipping:** It co-locates related information. If you Z-Order by `customer_id`, rows for the same customer will be grouped into the same file. When you filter for that ID, Delta Lake skips the files that don't contain it by checking the min/max statistics in the log.\n",
    "* **High Cardinality Columns:** Z-Ordering is most effective on columns that have many unique values (like `ID`, `email`, or `phone_number`) and are frequently used in `WHERE` clauses or as `JOIN` keys.\n",
    "* **The 4-Column Limit:** Effectiveness drops if you Z-Order by too many columns (usually stay under 1–4). It is a \"trade-off\" tool—the more columns you Z-Order, the less \"tight\" the clustering becomes for each one.\n",
    "\n",
    "##### **Summary Comparison**\n",
    "\n",
    "| Feature | OPTIMIZE | ZORDER |\n",
    "| --- | --- | --- |\n",
    "| **Primary Goal** | Solve the \"Small File Problem\" | Enable advanced \"Data Skipping\" |\n",
    "| **What it does** | Merges small files into ~1GB files | Clusters related row values together |\n",
    "| **When to run** | After many small writes/updates | On large tables with frequent filters |\n",
    "| **Resource Cost** | Low to Moderate (I/O heavy) | High (Compute intensive sorting) |\n",
    "\n",
    "\n",
    "##### **PySpark & SQL Usage**\n",
    "\n",
    "```sql\n",
    "-- SQL Syntax\n",
    "OPTIMIZE my_table \n",
    "ZORDER BY (customer_id, event_type);\n",
    "\n",
    "```\n",
    "\n",
    "```python\n",
    "# PySpark Syntax\n",
    "from delta.tables import DeltaTable\n",
    "deltaTable = DeltaTable.forPath(spark, \"/path/to/table\")\n",
    "deltaTable.optimize().zorderBy(\"customer_id\").executeZOrderBy()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e56511f9-b10e-4d89-b6bd-833efffabda0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "228a7631-8b45-4984-ad21-b0e53c7bf57d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **_4. Caching techniques_**\n",
    "In Databricks, caching is not a single \"on/off\" switch. Instead, there are several distinct layers of caching that serve different purposes. Understanding which one to use can save you significant time and compute costs.\n",
    "\n",
    "##### **1. Disk Cache (Formerly Delta Cache)**\n",
    "\n",
    "This is the most important cache for Databricks performance. It stores copies of remote Parquet/Delta files on the **local SSDs** of your worker nodes.\n",
    "\n",
    "* **How it works:** It automatically caches data as it is read from cloud storage. Subsequent reads of the same data are fetched from the local SSD instead of the network.\n",
    "* **Best Practice:** Choose a **\"Delta Cache Accelerated\"** worker type (like the `L-series` in Azure or `i-series` in AWS). These come with pre-configured SSDs specifically for this purpose.\n",
    "* **Manual Control:** You can force a cache warm-up using `CACHE SELECT * FROM table_name`.\n",
    "\n",
    "##### **2. Spark Cache (`.cache()` and `.persist()`)**\n",
    "\n",
    "This is the traditional Spark mechanism that stores data in **JVM Memory (RAM)**.\n",
    "\n",
    "* **Difference:** Unlike the Disk Cache (which stores raw file data), Spark Cache stores **computed DataFrames**. It is useful if you have a massive transformation (like a 10-way join) that you need to reuse multiple times in the same notebook.\n",
    "* **Storage Levels:** `.cache()` uses `MEMORY_AND_DISK` by default. `.persist()` allows you to choose `DISK_ONLY` or `MEMORY_ONLY_SER` (serialized) to save space.\n",
    "* **Warning:** Modern Databricks best practices suggest **avoiding Spark Caching** unless you are reusing an expensive intermediate result. The Disk Cache is usually faster and doesn't steal RAM from your Spark executors.\n",
    "\n",
    "##### **3. Query Result Cache**\n",
    "\n",
    "This is specific to **Databricks SQL Warehouses**.\n",
    "\n",
    "* **How it works:** It stores the final results of a SQL query. If you (or anyone else in your workspace) run the exact same query again and the underlying data hasn't changed, Databricks returns the result **instantly** without spinning up the cluster.\n",
    "* **Remote Result Cache:** In Serverless SQL, this cache is persistent across warehouse restarts, meaning you don't lose the \"warm\" results even if the warehouse shuts down at night.\n",
    "\n",
    "##### **Summary Comparison Table**\n",
    "\n",
    "| Feature | Disk Cache (Delta) | Spark Cache (`.cache`) | Result Cache |\n",
    "| --- | --- | --- | --- |\n",
    "| **Storage Medium** | Local SSD | RAM / JVM Memory | In-memory & Remote |\n",
    "| **Stores...** | Raw file data (Parquet) | Transformed DataFrames | Final Query Results |\n",
    "| **Trigger** | Automatic (on read) | Manual (`.cache()`) | Automatic (on SQL query) |\n",
    "| **Best For...** | General query speedup | Iterative ML / Multi-use DFs | BI Dashboards |\n",
    "\n",
    "##### **Performance Tip: Cache Invalidation**\n",
    "\n",
    "You don't need to manually clear the Disk Cache. Databricks **automatically detects** if the underlying Delta table has been updated and will evict the old files. For the Spark Cache, however, you should always call `.unpersist()` when you are finished to free up RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8addbe16-6bbb-4ca7-b2cc-0668ba62bca3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20f23c91-cbe8-4f2e-9e7e-9f255237199a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Practice**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d331fbd4-7ff3-4c75-aedd-f74d26bf460a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### ***1. Analyze query plans***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "692c1ff2-6b19-4f8b-b0ed-f0072cb57497",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n'Project [*]\n+- 'Filter ('event_type = purchase)\n   +- 'UnresolvedRelation [workspace, silver, ecommerce_cleaned], [], false\n\n== Analyzed Logical Plan ==\nevent_time: timestamp, event_type: string, product_id: int, category_id: bigint, category_code: string, brand: string, price: double, user_id: int, user_session: string, ingestion_ts: timestamp, event_date: date, price_tier: string\nProject [event_time#13527, event_type#13528, product_id#13529, category_id#13530L, category_code#13531, brand#13532, price#13533, user_id#13534, user_session#13535, ingestion_ts#13536, event_date#13537, price_tier#13538]\n+- Filter (event_type#13528 = purchase)\n   +- SubqueryAlias workspace.silver.ecommerce_cleaned\n      +- Relation workspace.silver.ecommerce_cleaned[event_time#13527,event_type#13528,product_id#13529,category_id#13530L,category_code#13531,brand#13532,price#13533,user_id#13534,user_session#13535,ingestion_ts#13536,event_date#13537,price_tier#13538] parquet\n\n== Optimized Logical Plan ==\nFilter (isnotnull(event_type#13528) AND (event_type#13528 = purchase))\n+- Relation workspace.silver.ecommerce_cleaned[event_time#13527,event_type#13528,product_id#13529,category_id#13530L,category_code#13531,brand#13532,price#13533,user_id#13534,user_session#13535,ingestion_ts#13536,event_date#13537,price_tier#13538] parquet\n\n== Physical Plan ==\n*(1) ColumnarToRow\n+- PhotonResultStage\n   +- PhotonScan parquet workspace.silver.ecommerce_cleaned[event_time#13527,event_type#13528,product_id#13529,category_id#13530L,category_code#13531,brand#13532,price#13533,user_id#13534,user_session#13535,ingestion_ts#13536,event_date#13537,price_tier#13538] DataFilters: [isnotnull(event_type#13528), (event_type#13528 = purchase)], DictionaryFilters: [(event_type#13528 = purchase)], Format: parquet, Location: PreparedDeltaFileIndex(1 paths)[s3://dbstorage-prod-bmylb/uc/ae6e4cfa-4597-46fb-a4e9-eeb27a5fb90a..., OptionalDataFilters: [], PartitionFilters: [], ReadSchema: struct<event_time:timestamp,event_type:string,product_id:int,category_id:bigint,category_code:str..., RequiredDataFilters: [isnotnull(event_type#13528), (event_type#13528 = purchase)]\n\n== Photon Explanation ==\nThe query is fully supported by Photon.\n== Optimizer Statistics (table names per statistics state) ==\n  missing = \n  partial = \n  full    = ecommerce_cleaned\n\n"
     ]
    }
   ],
   "source": [
    "# Explain query\n",
    "spark.sql(\"SELECT * FROM workspace.silver.ecommerce_cleaned WHERE event_type='purchase'\").explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1752144a-9d61-48c4-abd7-a509f5401876",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>plan</th></tr></thead><tbody><tr><td>== Parsed Logical Plan ==\n",
       "'Project [*]\n",
       "+- 'UnresolvedRelation [workspace, silver, ecommerce_cleaned], [], false\n",
       "\n",
       "== Analyzed Logical Plan ==\n",
       "event_time: timestamp, event_type: string, product_id: int, category_id: bigint, category_code: string, brand: string, price: double, user_id: int, user_session: string, ingestion_ts: timestamp, event_date: date, price_tier: string\n",
       "Project [event_time#13574, event_type#13575, product_id#13576, category_id#13577L, category_code#13578, brand#13579, price#13580, user_id#13581, user_session#13582, ingestion_ts#13583, event_date#13584, price_tier#13585]\n",
       "+- SubqueryAlias workspace.silver.ecommerce_cleaned\n",
       "   +- Relation workspace.silver.ecommerce_cleaned[event_time#13574,event_type#13575,product_id#13576,category_id#13577L,category_code#13578,brand#13579,price#13580,user_id#13581,user_session#13582,ingestion_ts#13583,event_date#13584,price_tier#13585] parquet\n",
       "\n",
       "== Optimized Logical Plan ==\n",
       "Relation workspace.silver.ecommerce_cleaned[event_time#13574,event_type#13575,product_id#13576,category_id#13577L,category_code#13578,brand#13579,price#13580,user_id#13581,user_session#13582,ingestion_ts#13583,event_date#13584,price_tier#13585] parquet\n",
       "\n",
       "== Physical Plan ==\n",
       "*(1) ColumnarToRow\n",
       "+- PhotonResultStage\n",
       "   +- PhotonScan parquet workspace.silver.ecommerce_cleaned[event_time#13574,event_type#13575,product_id#13576,category_id#13577L,category_code#13578,brand#13579,price#13580,user_id#13581,user_session#13582,ingestion_ts#13583,event_date#13584,price_tier#13585] DataFilters: [], DictionaryFilters: [], Format: parquet, Location: PreparedDeltaFileIndex(1 paths)[s3://dbstorage-prod-bmylb/uc/ae6e4cfa-4597-46fb-a4e9-eeb27a5fb90a..., OptionalDataFilters: [], PartitionFilters: [], ReadSchema: struct<event_time:timestamp,event_type:string,product_id:int,category_id:bigint,category_code:str..., RequiredDataFilters: []\n",
       "\n",
       "== Photon Explanation ==\n",
       "The query is fully supported by Photon.\n",
       "== Optimizer Statistics (table names per statistics state) ==\n",
       "  missing = \n",
       "  partial = \n",
       "  full    = ecommerce_cleaned\n",
       "</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "== Parsed Logical Plan ==\n'Project [*]\n+- 'UnresolvedRelation [workspace, silver, ecommerce_cleaned], [], false\n\n== Analyzed Logical Plan ==\nevent_time: timestamp, event_type: string, product_id: int, category_id: bigint, category_code: string, brand: string, price: double, user_id: int, user_session: string, ingestion_ts: timestamp, event_date: date, price_tier: string\nProject [event_time#13574, event_type#13575, product_id#13576, category_id#13577L, category_code#13578, brand#13579, price#13580, user_id#13581, user_session#13582, ingestion_ts#13583, event_date#13584, price_tier#13585]\n+- SubqueryAlias workspace.silver.ecommerce_cleaned\n   +- Relation workspace.silver.ecommerce_cleaned[event_time#13574,event_type#13575,product_id#13576,category_id#13577L,category_code#13578,brand#13579,price#13580,user_id#13581,user_session#13582,ingestion_ts#13583,event_date#13584,price_tier#13585] parquet\n\n== Optimized Logical Plan ==\nRelation workspace.silver.ecommerce_cleaned[event_time#13574,event_type#13575,product_id#13576,category_id#13577L,category_code#13578,brand#13579,price#13580,user_id#13581,user_session#13582,ingestion_ts#13583,event_date#13584,price_tier#13585] parquet\n\n== Physical Plan ==\n*(1) ColumnarToRow\n+- PhotonResultStage\n   +- PhotonScan parquet workspace.silver.ecommerce_cleaned[event_time#13574,event_type#13575,product_id#13576,category_id#13577L,category_code#13578,brand#13579,price#13580,user_id#13581,user_session#13582,ingestion_ts#13583,event_date#13584,price_tier#13585] DataFilters: [], DictionaryFilters: [], Format: parquet, Location: PreparedDeltaFileIndex(1 paths)[s3://dbstorage-prod-bmylb/uc/ae6e4cfa-4597-46fb-a4e9-eeb27a5fb90a..., OptionalDataFilters: [], PartitionFilters: [], ReadSchema: struct<event_time:timestamp,event_type:string,product_id:int,category_id:bigint,category_code:str..., RequiredDataFilters: []\n\n== Photon Explanation ==\nThe query is fully supported by Photon.\n== Optimizer Statistics (table names per statistics state) ==\n  missing = \n  partial = \n  full    = ecommerce_cleaned\n"
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "plan",
            "nullable": true,
            "type": "string"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 7
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "plan",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "explain extended select * from workspace.silver.ecommerce_cleaned "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7d72dbb-51a0-484b-b634-f344171b96a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fcf58c7f-41b6-4bcd-b1ab-21d96a61a883",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### ***2. Partition large tables***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf7bd2ff-0faa-4f88-96aa-57e58ae83e81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[num_affected_rows: bigint, num_inserted_rows: bigint]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "  CREATE TABLE silver.events_part\n",
    "  USING DELTA\n",
    "  PARTITIONED BY (event_date, event_type)\n",
    "  AS SELECT * FROM workspace.silver.ecommerce_cleaned\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53fa0804-c445-4033-b00b-17a2109758d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "367d52ed-7636-4a41-b73b-b464f6d40ae8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### ***3. Apply ZORDER***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b58fa8ac-3fe9-4c16-a5c9-fe37564cacd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[path: string, metrics: struct<numFilesAdded:bigint,numFilesRemoved:bigint,filesAdded:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,filesRemoved:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,partitionsOptimized:bigint,zOrderStats:struct<strategyName:string,inputCubeFiles:struct<num:bigint,size:bigint>,inputOtherFiles:struct<num:bigint,size:bigint>,inputNumCubes:bigint,mergedFiles:struct<num:bigint,size:bigint>,numOutputCubes:bigint,mergedNumCubes:bigint>,clusteringStats:struct<inputZCubeFiles:struct<numFiles:bigint,size:bigint>,inputOtherFiles:struct<numFiles:bigint,size:bigint>,inputNumZCubes:bigint,mergedFiles:struct<numFiles:bigint,size:bigint>,numOutputZCubes:bigint>,numBins:bigint,numBatches:bigint,totalConsideredFiles:bigint,totalFilesSkipped:bigint,preserveInsertionOrder:boolean,numFilesSkippedToReduceWriteAmplification:bigint,numBytesSkippedToReduceWriteAmplification:bigint,startTimeMs:bigint,endTimeMs:bigint,totalClusterParallelism:bigint,totalScheduledTasks:bigint,autoCompactParallelismStats:struct<maxClusterActiveParallelism:bigint,minClusterActiveParallelism:bigint,maxSessionActiveParallelism:bigint,minSessionActiveParallelism:bigint>,deletionVectorStats:struct<numDeletionVectorsRemoved:bigint,numDeletionVectorRowsRemoved:bigint>,recompressionCodec:string,numTableColumns:bigint,numTableColumnsWithStats:bigint,totalTaskExecutionTimeMs:bigint,skippedArchivedFiles:bigint,clusteringMetrics:struct<sizeOfTableInBytesBeforeLazyClustering:bigint,isNewMetadataCreated:boolean,isPOTriggered:boolean,isFull:boolean,approxClusteringQuality:double,approxClusteringQualityPerColumn:array<double>,approxClusteringCoverage:double,compactionType:string,numFilesSkippedWithoutStats:bigint,numFilesClassifiedToIntermediateNodes:bigint,sizeOfFilesClassifiedToIntermediateNodesInBytes:bigint,logicalSizeOfFilesClassifiedToIntermediateNodesInBytes:bigint,numFilesClassifiedToLeafNodes:bigint,sizeOfFilesClassifiedToLeafNodesInBytes:bigint,logicalSizeOfFilesClassifiedToLeafNodesInBytes:bigint,numThreadsForClassifier:int,clusterThresholdStrategy:string,minFileSize:bigint,maxFileSize:bigint,nodeMinNumFilesToCompact:bigint,numIdealFiles:bigint,numIdealFilesWithTrimmedStringMaxValue:bigint,numAddedFilesWithSameMinMaxOnClusteringColumns:array<bigint>,numClusteringTasksPlanned:int,numClusteringTasksNotPlannedDueToPO:int,numCompactionTasksPlanned:int,numCompactionTasksPlannedUndoneDueToPO:int,numOptimizeBatchesPlanned:int,numLeafNodesExpanded:bigint,numLeafNodesClustered:bigint,numGetFilesForNodeCalls:bigint,numSamplingJobs:bigint,numLeafNodesCompacted:bigint,numLeafNodesCompactedUndoneDueToPO:bigint,numIntermediateNodesCompacted:bigint,numIntermediateNodesCompactedUndoneDueToPO:bigint,totalSizeOfDataToCompactInBytes:bigint,totalSizeOfDataToCompactInBytesUndoneDueToPO:bigint,totalLogicalSizeOfDataToCompactInBytes:bigint,totalLogicalSizeOfDataToCompactInBytesUndoneDueToPO:bigint,numIntermediateNodesClustered:bigint,numFilesSkippedAfterExpansion:bigint,totalSizeOfFilesSkippedAfterExpansionInBytes:bigint,totalLogicalSizeOfFilesSkippedAfterExpansionInBytes:bigint,totalSizeOfDataToRewriteInBytes:bigint,totalLogicalSizeOfDataToRewriteInBytes:bigint,timeMetrics:struct<classifierTimeMs:bigint,optimizerTimeMs:bigint,metadataLoadTimeMs:bigint,totalGetFilesForNodeCallsTimeMs:bigint,totalSamplingTimeMs:bigint,metadataCreationTimeMs:bigint>,maxOptimizeBatchesInParallel:bigint,currentIteration:int,maxIterations:int,clusteringStrategy:string>>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"OPTIMIZE silver.events_part ZORDER BY (user_id, product_id)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3171eb35-31b1-4755-a08a-f80fc0ff39a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54adde07-0882-40a1-bb4d-4f7bf0260921",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### ***4. Benchmark improvements***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38fe17aa-9d89-4370-a3d6-cc582aa41fd2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 0.76s\n"
     ]
    }
   ],
   "source": [
    "# Benchmark\n",
    "import time\n",
    "start = time.time()\n",
    "spark.sql(\"SELECT * FROM workspace.silver.ecommerce_cleaned WHERE user_id=12345\").count()\n",
    "print(f\"Time: {time.time()-start:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49b7e531-86e6-497b-a178-6bf8cd8f1af9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Resources**\n",
    "- [Performance Tuning](https://docs.databricks.com/performance/)\n",
    "- [Optimization guide](https://docs.databricks.com/delta/optimizations-oss.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e6eb3fc-5a0b-4de3-a400-fcc80f5b2d9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "----"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5456446880004678,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Day_10",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}